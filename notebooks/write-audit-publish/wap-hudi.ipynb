{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e13cd9",
   "metadata": {},
   "source": [
    "<img src=\"https://hudi.apache.org/assets/images/hudi.png\"> &nbsp; &nbsp; &nbsp;<img src=\"../images/logo.svg\" alt=\"lakeFS logo\" width=300/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78797e22",
   "metadata": {},
   "source": [
    "## Write-Audit-Publish (WAP) pattern with Apache Hudi and lakeFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f89a51c",
   "metadata": {},
   "source": [
    "Please see the accompanying blog series for more details: \n",
    "\n",
    "1. [Data Engineering Patterns: Write-Audit-Publish (WAP)](https://lakefs.io/blog/data-engineering-patterns-write-audit-publish)\n",
    "1. [How to Implement Write-Audit-Publish (WAP)](https://lakefs.io/blog/how-to-implement-write-audit-publish)\n",
    "1. [Putting the Write-Audit-Publish Pattern into Practice with lakeFS](https://lakefs.io/blog/write-audit-publish-with-lakefs/)\n",
    "\n",
    "[@rmoff](https://twitter.com/rmoff/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89718426",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f824b929",
   "metadata": {},
   "source": [
    "## Set up the connection to lakeFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2e44584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_client.client import LakeFSClient\n",
    "\n",
    "lakefs_config = lakefs_client.Configuration()\n",
    "lakefs_config.username = 'AKIAIOSFODNN7EXAMPLE'\n",
    "lakefs_config.password = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'\n",
    "lakefs_config.host = 'http://lakefs:8000'\n",
    "\n",
    "lakefs = LakeFSClient(lakefs_config)\n",
    "lakefs_api_client = lakefs_client.ApiClient(lakefs_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0047139f",
   "metadata": {},
   "source": [
    "### Get the first repository present in lakeFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb017136",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lakeFS repository 'example' with storage namespace s3://example\n"
     ]
    }
   ],
   "source": [
    "repo=lakefs.repositories.list_repositories().results[0]\n",
    "print(f\"Using lakeFS repository '{repo.id}' with storage namespace {repo.storage_namespace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517f0ac",
   "metadata": {},
   "source": [
    "### Define the data storage directory based on the provided namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b647611",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using s3a://example for data storage\n"
     ]
    }
   ],
   "source": [
    "data_dir=repo.storage_namespace.replace('s3','s3a')\n",
    "print(f\"Using {data_dir} for data storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e906a",
   "metadata": {},
   "source": [
    "## Set up Spark "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c501bd",
   "metadata": {},
   "source": [
    "Added the the following to fix `java.lang.IllegalArgumentException: For input string: \"null\"` when querying a Hudi table per [8061](https://github.com/apache/hudi/issues/8061): \n",
    "    \n",
    "* `spark.hadoop.spark.sql.legacy.parquet.nanosAsLong`\n",
    "* `spark.hadoop.spark.sql.parquet.binaryAsString`\n",
    "* `spark.hadoop.spark.sql.parquet.int96AsTimestamp`\n",
    "* `spark.hadoop.spark.sql.caseSensitive`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e89a1b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://eb609d7ee688:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lakeFS / Jupyter</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff9c1dc520>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"lakeFS / Jupyter\") \\\n",
    "        .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://lakefs:8000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"AKIAIOSFODNN7EXAMPLE\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hudi:hudi-spark3.3-bundle_2.12:0.13.0\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\") \\\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\") \\\n",
    "        .config(\"spark.hadoop.spark.sql.legacy.parquet.nanosAsLong\", \"false\") \\\n",
    "        .config(\"spark.hadoop.spark.sql.parquet.binaryAsString\", \"false\") \\\n",
    "        .config(\"spark.hadoop.spark.sql.parquet.int96AsTimestamp\", \"true\") \\\n",
    "        .config(\"spark.hadoop.spark.sql.caseSensitive\", \"false\") \\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "spark\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceeaa9b",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad496308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"inferSchema\",\"true\").option(\"multiline\",\"true\").json(\"/home/jovyan/data/nyc_film_permits.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c344952b",
   "metadata": {},
   "source": [
    "### Inspect test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "488be3a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"permits_src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "925ca945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>borough</th>\n",
       "        <th>permit_cnt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Queens</td>\n",
       "        <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Brooklyn</td>\n",
       "        <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Staten Island</td>\n",
       "        <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Manhattan</td>\n",
       "        <td>463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Bronx</td>\n",
       "        <td>28</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------------+------------+\n",
       "|       borough | permit_cnt |\n",
       "+---------------+------------+\n",
       "|        Queens |        168 |\n",
       "|      Brooklyn |        334 |\n",
       "| Staten Island |          7 |\n",
       "|     Manhattan |        463 |\n",
       "|         Bronx |         28 |\n",
       "+---------------+------------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT borough, count(*) permit_cnt\n",
    "FROM permits_src\n",
    "GROUP BY borough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33b07f9",
   "metadata": {},
   "source": [
    "## Write test data to Hudi files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604b36e",
   "metadata": {},
   "source": [
    "### Set Hudi options\n",
    "\n",
    "_Hudi requires a Primary key for the table, so we're picking a composite key here since there's no obvious single field to use_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "084d9a79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hudi_options = {\n",
    "    'hoodie.table.name': 'permits',\n",
    "    'hoodie.datasource.write.recordkey.field': 'borough,startdatetime',\n",
    "    'hoodie.datasource.write.partitionpath.field': 'borough',\n",
    "    'hoodie.datasource.write.table.name': 'permits',\n",
    "    'hoodie.datasource.write.operation': 'insert'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71560028",
   "metadata": {},
   "source": [
    "### Write Hudi file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d418d02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "branch='main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d01d5be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "permits=(f\"{data_dir}/{branch}/nyc/permits\")\n",
    "\n",
    "df.write.format(\"hudi\"). \\\n",
    "    options(**hudi_options). \\\n",
    "    mode(\"overwrite\"). \\\n",
    "    save(permits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1786c706",
   "metadata": {},
   "source": [
    "### Inspect the files written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "369ce348",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggs/agg_plot/_delta_log/\n",
      "aggs/agg_plot/_delta_log/00000000000000000000.json\n",
      "aggs/agg_plot/_delta_log/00000000000000000001.json\n",
      "aggs/agg_plot/part-00000-2ee8ce47-d6e9-4fa9-a1ff-753028a42a84-c000.snappy.parquet\n",
      "aggs/agg_plot/part-00000-f9eda370-5b4d-4723-9196-4d8e40990f5d-c000.snappy.parquet\n",
      "aggs/agg_variety/_delta_log/\n",
      "aggs/agg_variety/_delta_log/00000000000000000000.json\n",
      "aggs/agg_variety/_delta_log/00000000000000000001.json\n",
      "aggs/agg_variety/part-00000-cf6566ff-3b49-499a-a8ea-0fab940e1174-c000.snappy.parquet\n",
      "aggs/agg_variety/part-00000-e3f85d05-1461-4d0c-b089-ff67dec276a2-c000.snappy.parquet\n",
      "nyc/permits/\n",
      "nyc/permits/.hoodie/\n",
      "nyc/permits/.hoodie/.aux/\n",
      "nyc/permits/.hoodie/.aux/.bootstrap/.fileids/\n",
      "nyc/permits/.hoodie/.aux/.bootstrap/.partitions/\n",
      "nyc/permits/.hoodie/.schema/\n",
      "nyc/permits/.hoodie/.temp/\n",
      "nyc/permits/.hoodie/20230518111510100.commit\n",
      "nyc/permits/.hoodie/20230518111510100.commit.requested\n",
      "nyc/permits/.hoodie/20230518111510100.inflight\n",
      "nyc/permits/.hoodie/archived/\n",
      "nyc/permits/.hoodie/hoodie.properties\n",
      "nyc/permits/.hoodie/metadata/\n",
      "nyc/permits/.hoodie/metadata/.hoodie/\n",
      "nyc/permits/.hoodie/metadata/.hoodie/.aux/\n",
      "nyc/permits/.hoodie/metadata/.hoodie/.aux/.bootstrap/.fileids/\n",
      "nyc/permits/.hoodie/metadata/.hoodie/.aux/.bootstrap/.partitions/\n",
      "nyc/permits/.hoodie/metadata/.hoodie/.schema/\n",
      "nyc/permits/.hoodie/metadata/.hoodie/.temp/\n",
      "nyc/permits/.hoodie/metadata/.hoodie/00000000000000.deltacommit\n",
      "nyc/permits/.hoodie/metadata/.hoodie/00000000000000.deltacommit.inflight\n",
      "nyc/permits/.hoodie/metadata/.hoodie/00000000000000.deltacommit.requested\n",
      "nyc/permits/.hoodie/metadata/.hoodie/20230518111510100.deltacommit\n",
      "nyc/permits/.hoodie/metadata/.hoodie/20230518111510100.deltacommit.inflight\n",
      "nyc/permits/.hoodie/metadata/.hoodie/20230518111510100.deltacommit.requested\n",
      "nyc/permits/.hoodie/metadata/.hoodie/archived/\n",
      "nyc/permits/.hoodie/metadata/.hoodie/hoodie.properties\n",
      "nyc/permits/.hoodie/metadata/files/.files-0000_00000000000000.log.1_0-0-0\n",
      "nyc/permits/.hoodie/metadata/files/.files-0000_00000000000000.log.2_0-15-25\n",
      "nyc/permits/.hoodie/metadata/files/.hoodie_partition_metadata\n",
      "nyc/permits/Bronx/\n",
      "nyc/permits/Bronx/.hoodie_partition_metadata\n",
      "nyc/permits/Bronx/ce4ed3ba-6b32-4362-936d-572c2f435180-0_4-8-15_20230518111510100.parquet\n",
      "nyc/permits/Brooklyn/\n",
      "nyc/permits/Brooklyn/.hoodie_partition_metadata\n",
      "nyc/permits/Brooklyn/37f10a4b-1da0-4529-b3c8-5cf4caacdc40-0_2-8-13_20230518111510100.parquet\n",
      "nyc/permits/Manhattan/\n",
      "nyc/permits/Manhattan/.hoodie_partition_metadata\n",
      "nyc/permits/Manhattan/afa4d438-17bb-4e62-b4bf-72192953fccc-0_3-8-14_20230518111510100.parquet\n",
      "nyc/permits/Queens/\n",
      "nyc/permits/Queens/.hoodie_partition_metadata\n",
      "nyc/permits/Queens/92019a0f-8e81-4193-a4de-ed31f6668066-0_0-8-11_20230518111510100.parquet\n",
      "nyc/permits/Staten Island/\n",
      "nyc/permits/Staten Island/.hoodie_partition_metadata\n",
      "nyc/permits/Staten Island/4641e11e-4af6-4855-b463-013d21f728d1-0_1-8-12_20230518111510100.parquet\n",
      "raw/soil/_delta_log/\n",
      "raw/soil/_delta_log/00000000000000000000.json\n",
      "raw/soil/_delta_log/00000000000000000001.json\n",
      "raw/soil/part-00000-c5ec71e5-55a6-4a1b-a003-6f665c24cdcc-c000.snappy.parquet\n",
      "raw/soil/part-00000-d92c107f-67c7-4f49-8831-c1d2348faa47-c000.snappy.parquet\n",
      "src/dataset_info.txt\n",
      "src/morrow-plots_README.txt\n",
      "src/morrow-plots_v01_1888-2019_soil.csv\n",
      "src/morrow-plots_v01_2020-2021_soil.csv\n",
      "src/morrow-plots_v01_codebook.pdf\n",
      "src/morrow-plots_v01x_2020-2021_soil.csv\n"
     ]
    }
   ],
   "source": [
    "for f in lakefs.objects.list_objects(repo.id,'main').results:\n",
    "    print(f['path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545fb202",
   "metadata": {},
   "source": [
    "### Load the Hudi data as a view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d27cc68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Hudi table from s3a://example/main/nyc/permits into view `permits_main`\n"
     ]
    }
   ],
   "source": [
    "permits=(f\"{data_dir}/{branch}/nyc/permits\")\n",
    "print(f\"Reading Hudi table from {permits} into view `permits_{branch}`\")\n",
    "\n",
    "spark.read. \\\n",
    "format(\"hudi\"). \\\n",
    "options(**hudi_options). \\\n",
    "load(permits). \\\n",
    "createOrReplaceTempView(f\"permits_{branch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9969d6fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>borough</th>\n",
       "        <th>permit_cnt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Manhattan</td>\n",
       "        <td>463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Brooklyn</td>\n",
       "        <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Queens</td>\n",
       "        <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Bronx</td>\n",
       "        <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Staten Island</td>\n",
       "        <td>7</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------------+------------+\n",
       "|       borough | permit_cnt |\n",
       "+---------------+------------+\n",
       "|     Manhattan |        463 |\n",
       "|      Brooklyn |        334 |\n",
       "|        Queens |        168 |\n",
       "|         Bronx |         28 |\n",
       "| Staten Island |          7 |\n",
       "+---------------+------------+"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT borough, count(*) permit_cnt\n",
    "FROM permits_main\n",
    "GROUP BY borough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcdcb3c",
   "metadata": {},
   "source": [
    "## Commit the data to the `main` branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0078cc8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'committer': 'docker',\n",
       " 'creation_date': 1684408537,\n",
       " 'id': 'f310fed40883dac47878380ab6ac20568ba61dedb2ab85d7bea5adda8f9a7aa9',\n",
       " 'message': 'First commit of NYC Permit data',\n",
       " 'meta_range_id': '',\n",
       " 'metadata': {},\n",
       " 'parents': ['00b07af2b550fdee6a19437999806b67bd2aae894220881293b790207205a6c1']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_client = lakefs_client.ApiClient(lakefs_config)\n",
    "\n",
    "from lakefs_client.api import commits_api\n",
    "from lakefs_client.model.commit import Commit\n",
    "from lakefs_client.model.commit_creation import CommitCreation\n",
    "\n",
    "api_instance = commits_api.CommitsApi(api_client)\n",
    "commit_creation = CommitCreation(\n",
    "    message=\"First commit of NYC Permit data\"\n",
    ") \n",
    "\n",
    "\n",
    "api_instance.commit(repo.id, 'main', commit_creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c5e94",
   "metadata": {},
   "source": [
    "# The Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5342229a",
   "metadata": {},
   "source": [
    "lakeFS is based on branches (just like git). Branches are copy-on-write, making them 'cheap' in terms of storage. \n",
    "\n",
    "We're going to create a branch to write data to, audit it, and then merge it back if we're happy with the audit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fa39ade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "branch='etl_job_42'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a62f81b",
   "metadata": {},
   "source": [
    "### Create branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fecd1769",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f310fed40883dac47878380ab6ac20568ba61dedb2ab85d7bea5adda8f9a7aa9'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lakefs_client.api import branches_api\n",
    "from lakefs_client.model.branch_creation import BranchCreation\n",
    "\n",
    "api_instance = branches_api.BranchesApi(lakefs_api_client)\n",
    "branch_creation = BranchCreation(\n",
    "    name=branch,\n",
    "    source=\"main\",\n",
    ") \n",
    "\n",
    "api_instance.create_branch(repo.id, branch_creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886ac446",
   "metadata": {},
   "source": [
    "### Check that we still see the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0de8df98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Hudi table from s3a://example/etl_job_42/nyc/permits into view `permits_etl_job_42`\n",
      "+-------------+----------+\n",
      "|      borough|permit_cnt|\n",
      "+-------------+----------+\n",
      "|    Manhattan|       463|\n",
      "|     Brooklyn|       334|\n",
      "|       Queens|       168|\n",
      "|        Bronx|        28|\n",
      "|Staten Island|         7|\n",
      "+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "permits=(f\"{data_dir}/{branch}/nyc/permits\")\n",
    "vw=(f\"permits_{branch}\")\n",
    "print(f\"Reading Hudi table from {permits} into view `{vw}`\")\n",
    "\n",
    "spark.read. \\\n",
    "format(\"hudi\"). \\\n",
    "options(**hudi_options). \\\n",
    "load(permits). \\\n",
    "createOrReplaceTempView(f\"{vw}\")\n",
    "\n",
    "spark.sql(f\"SELECT borough, count(*) permit_cnt FROM {vw} GROUP BY borough\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddc4ff7",
   "metadata": {},
   "source": [
    "# Write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649cdbe4",
   "metadata": {},
   "source": [
    "## Load the data into a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b710545a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Hudi table from s3a://example/etl_job_42/nyc/permits into table `nyc_permits`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Reading Hudi table from {permits} into table `nyc_permits`\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS nyc_permits\")\n",
    "spark.sql(\"CREATE TABLE nyc_permits USING HUDI LOCATION '\"+ permits + \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "789751cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>namespace</th>\n",
       "        <th>tableName</th>\n",
       "        <th>isTemporary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>default</td>\n",
       "        <td>nyc_permits</td>\n",
       "        <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td></td>\n",
       "        <td>permits_etl_job_42</td>\n",
       "        <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td></td>\n",
       "        <td>permits_main</td>\n",
       "        <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td></td>\n",
       "        <td>permits_src</td>\n",
       "        <td>False</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-----------+--------------------+-------------+\n",
       "| namespace |          tableName | isTemporary |\n",
       "+-----------+--------------------+-------------+\n",
       "|   default |        nyc_permits |       False |\n",
       "|           | permits_etl_job_42 |       False |\n",
       "|           |       permits_main |       False |\n",
       "|           |        permits_src |       False |\n",
       "+-----------+--------------------+-------------+"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9633438",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>borough</th>\n",
       "        <th>permit_cnt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Manhattan</td>\n",
       "        <td>463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Brooklyn</td>\n",
       "        <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Queens</td>\n",
       "        <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Bronx</td>\n",
       "        <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Staten Island</td>\n",
       "        <td>7</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------------+------------+\n",
       "|       borough | permit_cnt |\n",
       "+---------------+------------+\n",
       "|     Manhattan |        463 |\n",
       "|      Brooklyn |        334 |\n",
       "|        Queens |        168 |\n",
       "|         Bronx |         28 |\n",
       "| Staten Island |          7 |\n",
       "+---------------+------------+"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT borough, count(*) permit_cnt\n",
    "FROM nyc_permits\n",
    "GROUP BY borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "291843b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "DELETE FROM nyc_permits\n",
    "WHERE borough='Manhattan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca56563d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>borough</th>\n",
       "        <th>permit_cnt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Brooklyn</td>\n",
       "        <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Queens</td>\n",
       "        <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Bronx</td>\n",
       "        <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Staten Island</td>\n",
       "        <td>7</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------------+------------+\n",
       "|       borough | permit_cnt |\n",
       "+---------------+------------+\n",
       "|      Brooklyn |        334 |\n",
       "|        Queens |        168 |\n",
       "|         Bronx |         28 |\n",
       "| Staten Island |          7 |\n",
       "+---------------+------------+"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT borough, count(*) permit_cnt\n",
    "FROM nyc_permits\n",
    "GROUP BY borough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c3a04",
   "metadata": {},
   "source": [
    "## Inspecting the staged/unpublished data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c09de33",
   "metadata": {},
   "source": [
    "### Staged/unpublished data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e5ba8",
   "metadata": {},
   "source": [
    "#### The changes are reflected in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "232a938a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Hudi table from s3a://example/etl_job_42/nyc/permits into view `permits_etl_job_42`\n",
      "+-------------+----------+\n",
      "|      borough|permit_cnt|\n",
      "+-------------+----------+\n",
      "|     Brooklyn|       334|\n",
      "|       Queens|       168|\n",
      "|        Bronx|        28|\n",
      "|Staten Island|         7|\n",
      "+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "permits=(f\"{data_dir}/{branch}/nyc/permits\")\n",
    "vw=(f\"permits_{branch}\")\n",
    "print(f\"Reading Hudi table from {permits} into view `{vw}`\")\n",
    "\n",
    "spark.read. \\\n",
    "format(\"hudi\"). \\\n",
    "options(**hudi_options). \\\n",
    "load(permits). \\\n",
    "createOrReplaceTempView(f\"{vw}\")\n",
    "\n",
    "spark.sql(f\"SELECT borough, count(*) permit_cnt FROM {vw} GROUP BY borough\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e26a26",
   "metadata": {},
   "source": [
    "### Published data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5a4eb7",
   "metadata": {},
   "source": [
    "The data on the `main` branch remains unchanged. We can validate this by running a query against the data, specifying `main` as the branch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a2338a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Hudi table from s3a://example/main/nyc/permits into view `permits_main`\n",
      "+-------------+----------+\n",
      "|      borough|permit_cnt|\n",
      "+-------------+----------+\n",
      "|    Manhattan|       463|\n",
      "|     Brooklyn|       334|\n",
      "|       Queens|       168|\n",
      "|        Bronx|        28|\n",
      "|Staten Island|         7|\n",
      "+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "branch=\"main\"\n",
    "permits=(f\"{data_dir}/{branch}/nyc/permits\")\n",
    "vw=(f\"permits_{branch}\")\n",
    "print(f\"Reading Hudi table from {permits} into view `{vw}`\")\n",
    "\n",
    "spark.read. \\\n",
    "format(\"hudi\"). \\\n",
    "options(**hudi_options). \\\n",
    "load(permits). \\\n",
    "createOrReplaceTempView(f\"{vw}\")\n",
    "\n",
    "spark.sql(f\"SELECT borough, count(*) permit_cnt FROM {vw} GROUP BY borough\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58244fc",
   "metadata": {},
   "source": [
    "# Audit "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d8ce7",
   "metadata": {},
   "source": [
    "At the moment the data is written to the audit branch (`etl_job_42`), but not published to `main`. \n",
    "\n",
    "How you audit the data is up to you. The nice thing about the data being staged is that you can do it within the same ETL job, or have another tool do it.\n",
    "\n",
    "Here's a very simple example of doing in Python. We're going to programatically check that only the four expected boroughs remain in the data.\n",
    "\n",
    "First, we define those that are expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62d6b827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "expected_boroughs = {\"Queens\", \"Brooklyn\", \"Bronx\", \"Staten Island\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c05b9a5",
   "metadata": {},
   "source": [
    "Then we get a set of the actual boroughs in the staged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77abc608",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "branch=\"etl_job_42\"\n",
    "permits=(f\"{data_dir}/{branch}/nyc/permits\")\n",
    "distinct_boroughs = spark.read \\\n",
    "                    .format(\"hudi\") \\\n",
    "                    .load(permits) \\\n",
    "                    .select(\"borough\") \\\n",
    "                    .distinct() \\\n",
    "                    .toLocalIterator()\n",
    "boroughs = {row[0] for row in distinct_boroughs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81156579",
   "metadata": {},
   "source": [
    "Now we do two checks:\n",
    "\n",
    "1. Compare the length of the expected vs actual set\n",
    "2. Check that the two sets when unioned are still the same length. This is necessary, since the first test isn't sufficient alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c91668d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audit has passed üôåüèª\n"
     ]
    }
   ],
   "source": [
    "if (   (len(boroughs)          != len(expected_boroughs)) \\\n",
    "      or (len(boroughs)          != len(set.union(boroughs, expected_boroughs))) \\\n",
    "      or (len(expected_boroughs) != len(set.union(boroughs, expected_boroughs)))):\n",
    "    raise ValueError(f\"Audit failed, borough set does not match expected boroughs: {boroughs} != {expected_boroughs}\")\n",
    "else:\n",
    "    print(f\"Audit has passed üôåüèª\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19526bb",
   "metadata": {},
   "source": [
    "# Publish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc07949",
   "metadata": {},
   "source": [
    "Publishing data in lakeFS means merging the audit branch back into `main`, making it available to anyone working with the data in that branch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a637a54",
   "metadata": {},
   "source": [
    "## Commit the data to the audit branch (`etl_job_42`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b466db",
   "metadata": {},
   "source": [
    "We can add a commit message, as well as optional metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30ff1ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'committer': 'docker',\n",
       " 'creation_date': 1684408564,\n",
       " 'id': 'ab822f0217159c6bc952074135e3a48b5e24905ebcc4cd8bfd6bf6e7876006b4',\n",
       " 'message': 'Remove data for Manhattan from permits dataset',\n",
       " 'meta_range_id': '',\n",
       " 'metadata': {'author': 'rmoff', 'etl job name': 'etl_job_42'},\n",
       " 'parents': ['f310fed40883dac47878380ab6ac20568ba61dedb2ab85d7bea5adda8f9a7aa9']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lakefs_client.api import commits_api\n",
    "from lakefs_client.model.commit import Commit\n",
    "from lakefs_client.model.commit_creation import CommitCreation\n",
    "\n",
    "api_instance = commits_api.CommitsApi(api_client)\n",
    "commit_creation = CommitCreation(\n",
    "    message=\"Remove data for Manhattan from permits dataset\",\n",
    "    metadata={\n",
    "        \"etl job name\": \"etl_job_42\",\n",
    "        \"author\": \"rmoff\",\n",
    "    }\n",
    ") \n",
    "\n",
    "api_instance.commit(repo.id, branch, commit_creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdce995",
   "metadata": {},
   "source": [
    "## Merge the branch back into `main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd0375e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reference': 'e1906d3f2b37292b23720a80bc23512936ddbb3f7deff5f820436997c1d9c0a8',\n",
       " 'summary': {'added': 0, 'changed': 0, 'conflict': 0, 'removed': 0}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lakefs.refs.merge_into_branch(repository=repo.id, source_ref='etl_job_42', destination_branch='main')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38524dd",
   "metadata": {},
   "source": [
    "## Inspecting the published data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0e3ccda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Hudi table from s3a://example/main/nyc/permits into view `permits_main`\n",
      "+-------------+----------+\n",
      "|      borough|permit_cnt|\n",
      "+-------------+----------+\n",
      "|     Brooklyn|       334|\n",
      "|       Queens|       168|\n",
      "|        Bronx|        28|\n",
      "|Staten Island|         7|\n",
      "+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "branch=\"main\"\n",
    "permits=(f\"{data_dir}/{branch}/nyc/permits\")\n",
    "vw=(f\"permits_{branch}\")\n",
    "print(f\"Reading Hudi table from {permits} into view `{vw}`\")\n",
    "\n",
    "spark.read. \\\n",
    "format(\"hudi\"). \\\n",
    "options(**hudi_options). \\\n",
    "load(permits). \\\n",
    "createOrReplaceTempView(f\"{vw}\")\n",
    "\n",
    "spark.sql(f\"SELECT borough, count(*) permit_cnt FROM {vw} GROUP BY borough\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7482b2db",
   "metadata": {},
   "source": [
    "# Where Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814d43a5",
   "metadata": {},
   "source": [
    "* For more information about write-audit-publish see [this talk from Michelle Winters](https://www.youtube.com/watch?v=fXHdeBnpXrg&t=1001s) and [this talk from Sam Redai](https://www.dremio.com/wp-content/uploads/2022/05/Sam-Redai-The-Write-Audit-Publish-Pattern-via-Apache-Iceberg.pdf).\n",
    "* To try out lakeFS check out the [hands-on Quickstart](https://docs.lakefs.io/quickstart/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
