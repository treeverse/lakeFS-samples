{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install Python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:05.830869Z",
     "start_time": "2024-08-19T15:45:04.819700Z"
    },
    "is_executing": true,
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install onnx onnxruntime tf2onnx lakefs==0.7.1 s3fs==2024.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dependencies for the model training code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:08.983925Z",
     "start_time": "2024-08-19T15:45:05.835311Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "import tf2onnx\n",
    "import onnx\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import lakefs\n",
    "import os\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output might show TensorFlow messages, such as a \"Could not find TensorRT\" warning. You can ignore these messages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define lakeFS Storage and Repository information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs_storage_options={\n",
    "      \"key\": os.environ.get('LAKECTL_CREDENTIALS_ACCESS_KEY_ID'),\n",
    "      \"secret\": os.environ.get('LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY'),\n",
    "      \"client_kwargs\": {\n",
    "         \"endpoint_url\": os.environ.get('LAKECTL_SERVER_ENDPOINT_URL')\n",
    "      }\n",
    "}\n",
    "\n",
    "repo_name = os.environ.get('LAKEFS_REPO_NAME')\n",
    "mainBranch = \"main\"\n",
    "trainingBranch = \"train01\"\n",
    "\n",
    "repo = lakefs.Repository(repo_name)\n",
    "print(repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training branch in lakeFS and load the CSV data to the training branch in lakeFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "branchTraining = repo.branch(trainingBranch).create(source_reference=mainBranch, exist_ok=True)\n",
    "\n",
    "obj = branchTraining.object(path='data/train.csv')\n",
    "with open('data/train.csv', mode='rb') as reader, obj.writer(mode='wb', metadata={'using': 'python_wrapper', 'source':'Fraud Detection Demo'}) as writer:\n",
    "    writer.write(reader.read())\n",
    "\n",
    "obj = branchTraining.object(path='data/validate.csv')\n",
    "with open('data/validate.csv', mode='rb') as reader, obj.writer(mode='wb', metadata={'using': 'python_wrapper', 'source':'Fraud Detection Demo'}) as writer:\n",
    "    writer.write(reader.read())\n",
    "\n",
    "obj = branchTraining.object(path='data/test.csv')\n",
    "with open('data/test.csv', mode='rb') as reader, obj.writer(mode='wb', metadata={'using': 'python_wrapper', 'source':'Fraud Detection Demo'}) as writer:\n",
    "    writer.write(reader.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the CSV data from the training branch in lakeFS\n",
    "\n",
    "The CSV data that you use to train the model contains the following fields:\n",
    "\n",
    "* **distancefromhome** - The distance from home where the transaction happened.\n",
    "* **distancefromlast_transaction** - The distance from the last transaction that happened.\n",
    "* **ratiotomedianpurchaseprice** - The ratio of purchased price compared to median purchase price.\n",
    "* **repeat_retailer** - If it's from a retailer that already has been purchased from before.\n",
    "* **used_chip** - If the credit card chip was used.\n",
    "* **usedpinnumber** - If the PIN number was used.\n",
    "* **online_order** - If it was an online order.\n",
    "* **fraud** - If the transaction is fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:09.394745Z",
     "start_time": "2024-08-19T15:45:09.051361Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the input (X) and output (Y) data. \n",
    "# The only output data is whether it's fraudulent. All other fields are inputs to the model.\n",
    "\n",
    "feature_indexes = [\n",
    "    1,  # distance_from_last_transaction\n",
    "    2,  # ratio_to_median_purchase_price\n",
    "    4,  # used_chip\n",
    "    5,  # used_pin_number\n",
    "    6,  # online_order\n",
    "]\n",
    "\n",
    "label_indexes = [\n",
    "    7  # fraud\n",
    "]\n",
    "\n",
    "df = pd.read_csv(f\"s3://{repo_name}/{trainingBranch}/data/train.csv\", storage_options=lakefs_storage_options)\n",
    "X_train = df.iloc[:, feature_indexes].values\n",
    "y_train = df.iloc[:, label_indexes].values\n",
    "\n",
    "df = pd.read_csv(f\"s3://{repo_name}/{trainingBranch}/data/validate.csv\", storage_options=lakefs_storage_options)\n",
    "X_val = df.iloc[:, feature_indexes].values\n",
    "y_val = df.iloc[:, label_indexes].values\n",
    "\n",
    "df = pd.read_csv(f\"s3://{repo_name}/{trainingBranch}/data/test.csv\", storage_options=lakefs_storage_options)\n",
    "X_test = df.iloc[:, feature_indexes].values\n",
    "y_test = df.iloc[:, label_indexes].values\n",
    "\n",
    "\n",
    "# Scale the data to remove mean and have unit variance. The data will be between -1 and 1, which makes it a lot easier for the model to learn than random (and potentially large) values.\n",
    "# It is important to only fit the scaler to the training data, otherwise you are leaking information about the global distribution of variables (which is influenced by the test set) into the training set.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "obj = branchTraining.object(path='artifact/test_data.pkl')\n",
    "with obj.writer(\"wb\") as handle:\n",
    "    pickle.dump((X_test, y_test), handle)\n",
    "obj = branchTraining.object(path='artifact/scaler.pkl')\n",
    "with obj.writer(\"wb\") as handle:\n",
    "    pickle.dump(scaler, handle)\n",
    "\n",
    "# Since the dataset is unbalanced (it has many more non-fraud transactions than fraudulent ones), set a class weight to weight the few fraudulent transactions higher than the many non-fraud transactions.\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train.ravel())\n",
    "class_weights = {i : class_weights[i] for i in range(len(class_weights))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "The model is a simple, fully-connected, deep neural network, containing three hidden layers and one output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:09.489856Z",
     "start_time": "2024-08-19T15:45:09.419813Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=len(feature_indexes)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Training a model is often the most time-consuming part of the machine learning process.  Large models can take multiple GPUs for days.  Expect the training on CPU for this very simple model to take a minute or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:29.664796Z",
     "start_time": "2024-08-19T15:45:09.496686Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model and get performance\n",
    "import os\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "epochs = 2\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=True,\n",
    "    class_weight=class_weights\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Training of model is complete. Took {end-start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:29.845680Z",
     "start_time": "2024-08-19T15:45:29.674230Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Normally we use tf2.onnx.convert.from_keras.\n",
    "# workaround for tf2onnx bug https://github.com/onnx/tensorflow-onnx/issues/2348\n",
    "\n",
    "# Wrap the model in a `tf.function`\n",
    "@tf.function(input_signature=[tf.TensorSpec([None, X_train.shape[1]], tf.float32, name='dense_input')])\n",
    "def model_fn(x):\n",
    "    return model(x)\n",
    "\n",
    "# Convert the Keras model to ONNX\n",
    "model_proto, _ = tf2onnx.convert.from_function(\n",
    "    model_fn,\n",
    "    input_signature=[tf.TensorSpec([None, X_train.shape[1]], tf.float32, name='dense_input')]\n",
    ")\n",
    "\n",
    "# Save the model as ONNX for easy use of ModelMesh\n",
    "os.makedirs(\"models/fraud/1\", exist_ok=True)\n",
    "onnx.save(model_proto, \"models/fraud/1/model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output might include TensorFlow messages related to GPUs. You can ignore these messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm the model file was created successfully\n",
    "\n",
    "The output should include the model name, size, and date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.012353Z",
     "start_time": "2024-08-19T15:45:29.856416Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! ls -alRh ./models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.047040Z",
     "start_time": "2024-08-19T15:45:30.029773Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import pickle\n",
    "import onnxruntime as rt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test data and scaler from the training branch in lakeFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.062713Z",
     "start_time": "2024-08-19T15:45:30.058023Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "obj = branchTraining.object(path='artifact/scaler.pkl')\n",
    "with obj.reader('rb') as handle:\n",
    "    scaler = pickle.load(handle)\n",
    "obj = branchTraining.object(path='artifact/test_data.pkl')\n",
    "with obj.reader('rb') as handle:\n",
    "    (X_test, y_test) = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an ONNX inference runtime session and predict values for all test inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.210272Z",
     "start_time": "2024-08-19T15:45:30.073900Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = rt.InferenceSession(\"models/fraud/1/model.onnx\", providers=rt.get_available_providers())\n",
    "input_name = sess.get_inputs()[0].name\n",
    "output_name = sess.get_outputs()[0].name\n",
    "y_pred_temp = sess.run([output_name], {input_name: X_test.astype(np.float32)}) \n",
    "y_pred_temp = np.asarray(np.squeeze(y_pred_temp[0]))\n",
    "threshold = 0.95\n",
    "y_pred = np.where(y_pred_temp > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.644142Z",
     "start_time": "2024-08-19T15:45:30.221686Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "\n",
    "y_test_arr = y_test.squeeze()\n",
    "correct = np.equal(y_pred, y_test_arr).sum().item()\n",
    "acc = (correct / len(y_pred)) * 100\n",
    "precision = precision_score(y_test_arr, np.round(y_pred))\n",
    "recall = recall_score(y_test_arr, np.round(y_pred))\n",
    "\n",
    "print(f\"Eval Metrics: \\n Accuracy: {acc:>0.1f}%, \"\n",
    "      f\"Precision: {precision:.4f}, Recall: {recall:.4f} \\n\")\n",
    "\n",
    "c_matrix = confusion_matrix(y_test_arr, y_pred)\n",
    "ConfusionMatrixDisplay(c_matrix).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Is Sally's transaction likely to be fraudulent?\n",
    "\n",
    "Here is the order of the fields from Sally's transaction details:\n",
    "* distance_from_last_transaction\n",
    "* ratio_to_median_price\n",
    "* used_chip \n",
    "* used_pin_number\n",
    "* online_order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:45:30.679688Z",
     "start_time": "2024-08-19T15:45:30.669086Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sally_transaction_details = [\n",
    "    [0.3111400080477545,\n",
    "    1.9459399775518593,\n",
    "    1.0,\n",
    "    0.0,\n",
    "    0.0]\n",
    "    ]\n",
    "\n",
    "prediction = sess.run([output_name], {input_name: scaler.transform(sally_transaction_details).astype(np.float32)})\n",
    "\n",
    "print(\"Is Sally's transaction predicted to be fraudulent? (true = YES, false = NO) \")\n",
    "print(np.squeeze(prediction) > threshold)\n",
    "\n",
    "print(\"How likely was Sally's transaction to be fraudulent? \")\n",
    "print(\"{:.5f}\".format(100 * np.squeeze(prediction)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "63462a1f26ab486248b2a0fd058a0d9f9a6566a80083a3e1eb8f35617f2381b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
