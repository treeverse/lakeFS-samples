{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43e3aa9c-9a20-4468-9846-4c6f5902b7c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://lakefs.io/wp-content/uploads/2022/09/lakeFS-Logo.svg\" alt=\"lakeFS logo\" width=200/>\n",
    "\n",
    "# ML Data Version Control and Reproducibility at Scale\n",
    "\n",
    "### In the ever-evolving landscape of machine learning (ML), data stands as the cornerstone upon which triumphant models are built. However, as ML projects expand and encompass larger and more complex datasets, the challenge of efficiently managing and controlling data at scale becomes more pronounced.\n",
    "\n",
    "### Breaking Down Conventional Approaches:\n",
    "##### The Copy/Paste Predicament: In the world of data science, it's commonplace for data scientists to extract subsets of data to their local environments for model training. This method allows for iterative experimentation, but it introduces challenges that hinder the seamless evolution of ML projects.\n",
    "\n",
    "##### Reproducibility Constraints: Traditional practices of copying and modifying data locally lack the version control and audit-ability crucial for reproducibility. Iterating on models with various data subsets becomes a daunting task.\n",
    "\n",
    "##### Inefficient Data Transfer: Regularly shuttling data between the central repository and local environments strains resources and time, especially when choosing different subsets of data for each training run.\n",
    "\n",
    "\n",
    "In this sample, you'll learn how to use lakeFS for scalable data version control and reproducibility in ML workflows. The notebook demonstrates how to create branches for different experiments, work with data both locally, and efficiently manage large datasets on the cloud with no duplication. The demo will also cover integration with tools like DeltaLake, PyTorch, MinIO, and MLflow and how they ensure seamless data processing and experiment tracking using the medallion architecture. By the end, you'll be able to access the data directly in the lakeFS UI through a link provided in the MLflow UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1a8d403-ec02-4dcc-8fc3-38da45734113",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "source": [
    "## Target Architecture\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/inline-images/db-277-blog-img-3.png\" alt=\"target architecture\" width=800/>\n",
    "\n",
    "#### Source: Databricks Blogs:\n",
    "##### [Accelerating Your Deep Learning with PyTorch Lightning on Databricks](https://www.databricks.com/blog/2022/09/07/accelerating-your-deep-learning-pytorch-lightning-databricks.html)\n",
    "##### [Image Segmentation with Databricks](https://florent-brosse.medium.com/image-segmentation-with-databricks-6db19d23725d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### You can run this same notebook in local container. This picture explains the full procees:\n",
    "<img src=\"./files/Images/ImageSegmentation/ImageSegmentation.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f3e1e04-ef13-45c9-8d5a-d637080fc5af",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c377a867-3690-4c7e-b899-7f5d376c0bd8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### You can change repo name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1144e38-f35e-43eb-9435-a656f5560af5",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_name = \"image-segmentation-local-repo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9b7d4ff-2a76-40a4-8629-8818f667bfb8",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "**(you shouldn't need to change anything in this section, just run it)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddc2a99a-da97-441f-a734-663a6617f88f",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run ./ImageSegmentationSetup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an empty Git repository and configure Git. Git will version control your code while lakeFS will version control your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git init {repo_name}\n",
    "!git config --global user.email \"you@example.com\"\n",
    "!git config --global user.name \"Your Name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ed0c446-1d2f-4b0a-a59b-4b4e7e33ae1a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Main demo starts here üö¶ üëáüèª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b873d753-63a9-4146-bf81-22d4ffd3c23c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import training data to experiment branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a1d6958-521d-4fc8-b422-ac64e5b7b34a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create a branch for each experiment, as well as a Git branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf98d56-be45-4093-bff1-e6dea24c76a8",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experimentBranchN = experimentBranch+\"-1\"\n",
    "\n",
    "try:\n",
    "    repo.branch(experimentBranchN).head\n",
    "    branchExperimentBranchN = repo.branch(experimentBranchN)\n",
    "    print(f\"{experimentBranchN} already exists\")\n",
    "except NotFoundException as f:\n",
    "    if localOrDistributedComputing == \"LOCAL\":\n",
    "        !cd {repo_name} && git checkout -b {experimentBranchN}\n",
    "    branchExperimentBranchN = repo.branch(experimentBranchN).create(source_reference=emptyBranch)\n",
    "    print(f\"{experimentBranchN} branch created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66e5aeb9-d444-46ed-b45d-755e54642cd8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Import training data to lakeFS repo\n",
    "#### This is zero-copy operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfec7383-400e-4a21-aae6-aa0683c3350d",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import_images(file_list_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd319b42-6da9-42d0-b681-8e970527b9f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Clone experiment branch locally\n",
    "#### This will download images locally. You will notice \"image-segmentation-local-repo/lakefs_local\" folder in Jupyter File Browser on the left side panel. You can browse the files inside this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67daa1c8-db4f-4816-9969-3673bab22d28",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakeFSLocalCommand = f\"lakectl local clone lakefs://{repo.id}/{experimentBranchN}/ {repo_path}\"\n",
    "response = ! $lakeFSLocalCommand\n",
    "print_lakectl_response(response, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's review \".gitignore\" file and \".lakefs_ref.yaml\" file created by previous \"lakectl local clone\" command.\n",
    "#### You will notice in .gitignore file that Git will not commit any data files in \"lakefs_local\" folder but will commit \".lakefs_ref.yaml\" file which includes lakeFS commit information. This way code as well as commit information about data will be kept together in Git repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat {repo_name}/.gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat {repo_path}/.lakefs_ref.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete images smaller than 100KB in size locally. Add few new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!find {training_data_path} -type f -name \"*.jpg\" -size -100k -delete\n",
    "!cp /data/airbus-ship-detection/new-images/*.jpg {training_data_path}\n",
    "diff_branch(repo.id, repo_path, experimentBranchN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add changes to Git repo and perform initial commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd {repo_name} && git add -A && git status\n",
    "!cd {repo_name} && git commit -m \"Initial commit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit local changes to lakeFS repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "commitMessage = 'Deleted images smaller than 100KB in size and added few images'\n",
    "commit(repo.id, repo_path, experimentBranchN, commitMessage)\n",
    "lakefs_set_tag(repo.id, f\"{tagPrefix}-{experimentBranchN}-raw-images\", experimentBranchN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "145e8baf-6bb3-4c9b-807d-27120c6ee422",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Verify that you can read the local dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "895b0ea9-855f-4368-a6ac-b71a7e1ffbb3",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"image\").load(training_data_path)\n",
    "df.select(\"image.origin\", \"image.width\", \"image.height\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62d1893d-9d3a-4901-859d-137f55465ffa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Run the data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "goldDatasetTagID = data_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8276798-7707-44a4-afe1-d455811fe079",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Run the Image Segmentation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce0cb5aa-91c9-4241-8220-2f176bcdf158",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Split data as train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f469b84f-bcdd-4a6f-ab15-a730db89bba2",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gold_images_df = spark.read.format(\"delta\").load(f\"{gold_data_path}/{training_data_folder}\")\n",
    "(images_train, images_test) = gold_images_df.randomSplit(weights = [0.8, 0.2], seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e602614-98df-4ec3-a72f-3efcff223a6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Prepare the dataset in PyTorch format by using Petastorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e826d87f-5a38-46df-ae0a-f04f7a351b5f",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the converter cache folder to petastorm_path\n",
    "petastorm_path = 'file:///home/jovyan/petastorm/cache'\n",
    "\n",
    "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, petastorm_path)\n",
    "# convert the image for pytorch\n",
    "converter_train = make_spark_converter(images_train.coalesce(4)) # You can increase number of partitions from 4 if parquet file sizes generated by Petastorm are more than 50 MB\n",
    "converter_test = make_spark_converter(images_test.coalesce(4))\n",
    "print(f\"Images in training dataset: {len(converter_train)}, Images in test dataset: {len(converter_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61ccfd3a-743a-40cd-825e-d45a333d2760",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Train the base Model\n",
    "\n",
    "### Train the model with \"FPN\" architecture, \"resnet34\" encoder and learning rate of \"0.0001\"\n",
    "\n",
    "#### Model will return Intersection over Union (IoU) metric which is a widely-used evaluation metric in object detection and image segmentation tasks\n",
    "#### IoU measures the overlap between predicted bounding boxes and ground truth boxes, with scores ranging from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05cfc88d-6ecd-49fa-a1f5-be1821d76e42",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_per_image_iou = train_model(\"FPN\", \"resnet34\", 0.0001)\n",
    "print(f\"Intersection over Union (IoU) metric value: {valid_per_image_iou}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the base Model again with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05cfc88d-6ecd-49fa-a1f5-be1821d76e42",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_per_image_iou = train_model(\"FPN\", \"resnet50\", 0.0002)\n",
    "print(f\"Intersection over Union (IoU) metric value: {valid_per_image_iou}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e273b5a-f534-4a08-9b51-7270e3f7d73d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Save the best model to the MLflow registry (as a new version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbbdb268-4c0c-4e47-82ed-557303c616bf",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the best model from the registry\n",
    "best_model = \\\n",
    "mlflow.search_runs(filter_string='attributes.status = \"FINISHED\" and tags.lakefs_demos = \"image_segmentation\"',\n",
    "                   order_by=[\"metrics.valid_per_image_iou DESC\"], max_results=1).iloc[0]\n",
    "model_registered = mlflow.register_model(\"runs:/\" + best_model.run_id + \"/model\", \"lakefs_demos_image_segmentation\")\n",
    "print(model_registered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "928d2cd5-82e2-4880-9e6a-d68dff4910c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Save the best model information in the lakeFS repository\n",
    "\n",
    "#### Commit log in the lakeFS repository also includes URL to go to best registered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c24b4e76-0868-47fd-bfc0-633d7f509962",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "f = open(f\"{repo_path}/best_model.txt\", \"w\")\n",
    "f.write(best_model.to_string())\n",
    "f.close()\n",
    "\n",
    "commitMetadata = commit_metadata_for_best_model(best_model, model_registered)\n",
    "diff_branch(repo.id, repo_path, experimentBranchN)\n",
    "\n",
    "commitMessage = 'Information on best model'\n",
    "commit_id = commit(repo.id, repo_path, experimentBranchN, commitMessage, commitMetadata)\n",
    "lakefs_set_tag(repo.id, f\"{tagPrefix}-{experimentBranchN}-best-model\", experimentBranchN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy notebooks (code) to Git repo. The \"git add\" command adds changes in the working directory to the staging area.\n",
    "#### Git doesn't add data files to staging area while adds \".lakefs_ref.yaml\" file which includes lakeFS commit information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -t {repo_name} 'Image Segmentation.ipynb' 'ImageSegmentationSetup.ipynb'\n",
    "!cd {repo_name} && git add -A && git status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you want to access MLflow UI then open the [start-mlflow-ui](./start-mlflow-ui.ipynb) notebook, start MLflow server and go to [MLflow UI](http://127.0.0.1:5002/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run following cell to generate the hyperlink to go to the commit page in lakeFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "md(f\"<br/>üëâüèª **Go to [the commit page in lakeFS]({lakefsUIEndPoint}/repositories/{repo_name}/commits/{commit_id}) \\\n",
    "to see the commit made to the repository along with information for the best model.<br>Click on 'Open Registered Model UI' button on the commit page to \\\n",
    "open the best model in MLflow UI.<br>Click on 'Source Run' link in MLflow UI to get run details including model pickle file(python_model.pkl).**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing your data in lakeFS\n",
    "\n",
    "### Check out the GIF below to see the process of navigating from the MLflow UI to the lakeFS UI using a tagged commit link. The GIF demonstrates how to:\n",
    "\n",
    "#### Access the MLflow UI and locate the relevant tag.\n",
    "\n",
    "#### Use the tag to seamlessly switch to the lakeFS UI, from MLflow. \n",
    "\n",
    "#### View your data in its raw, bronze, silver, or gold form.\n",
    "\n",
    "### This makes it easy to track and analyze your data throughout the different stages of your ML workflow. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./files/images/ImageSegmentation/MLFlowLakeFS.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03ff8dba-f5d6-4aa9-8c93-7a74833abf3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## More Questions?\n",
    "\n",
    "[<img src=\"https://lakefs.io/wp-content/uploads/2023/06/Join-slack.svg\" alt=\"lakeFS logo\" width=700/>](https://lakefs.io/slack)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Image Segmentation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
