{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2c8fa0-1702-411a-b11c-3190679bf31c",
   "metadata": {},
   "source": [
    "# [Integration of lakeFS with Airflow](https://docs.lakefs.io/integrations/airflow.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24fa50-8557-4a9e-8463-f093ce8d2bf9",
   "metadata": {},
   "source": [
    "## Import Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bfe84c-fce2-4be0-8314-073c6b9aa1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "import os\n",
    "import time\n",
    "from airflow.models import DagBag\n",
    "import lakefs_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ede096-1b84-4b59-bdd1-2608ced6be51",
   "metadata": {},
   "source": [
    "## Working with the lakeFS Python client API\n",
    "\n",
    "###### Note: To learn more about lakeFS Python integration visit https://docs.lakefs.io/integrations/python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468e98e4-7e06-4373-b36b-f1763cc7755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xmode Minimal\n",
    "if not 'client' in locals():\n",
    "    import lakefs_client\n",
    "    from lakefs_client import models\n",
    "    from lakefs_client.client import LakeFSClient\n",
    "\n",
    "    # lakeFS credentials and endpoint\n",
    "    configuration = lakefs_client.Configuration()\n",
    "    configuration.username = lakefsAccessKey\n",
    "    configuration.password = lakefsSecretKey\n",
    "    configuration.host = lakefsEndPoint\n",
    "\n",
    "    client = LakeFSClient(configuration)\n",
    "    print(\"Created lakeFS client.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144747fc-cade-4929-8ae2-d488091ff273",
   "metadata": {},
   "source": [
    "## Verify lakeFS credentials by getting lakeFS version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20525e09-3b2a-4ac1-827f-c0d87a6b0f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verifying lakeFS credentialsâ€¦\")\n",
    "try:\n",
    "    v=client.config.get_config()\n",
    "except:\n",
    "    print(\"ðŸ›‘ failed to get lakeFS version\")\n",
    "else:\n",
    "    print(f\"â€¦âœ…lakeFS credentials verified\\n\\nâ„¹ï¸lakeFS version {v['version_config']['version']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83af336f-9866-4aca-9303-9a9171fa9fd9",
   "metadata": {},
   "source": [
    "## S3A Gateway configuration\n",
    "\n",
    "##### Note: lakeFS can be configured to work with Spark in two ways:\n",
    "###### * Access lakeFS using the S3A gateway https://docs.lakefs.io/integrations/spark.html#access-lakefs-using-the-s3a-gateway.\n",
    "###### * Access lakeFS using the lakeFS-specific Hadoop FileSystem https://docs.lakefs.io/integrations/spark.html#access-lakefs-using-the-lakefs-specific-hadoop-filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c2624-3ae6-464b-9379-65198a1218c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", lakefsAccessKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", lakefsSecretKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", lakefsEndPoint)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669f0f3a-da95-4c09-b8b5-4c00380ce913",
   "metadata": {},
   "source": [
    "## Start Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b436379d-bb36-4022-905f-4c3aeff77fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./airflow/airflow-webserver.pid'):\n",
    "    print(\"Airflow is running\")\n",
    "else:\n",
    "    print(\"Starting Airflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ee38e-49b1-423b-8577-d3e802518edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --bg --out script_out --err script_error\n",
    "FILE=./airflow/airflow-webserver.pid\n",
    "if test -f \"$FILE\"; then\n",
    "    echo \"Airflow Webserver is running\"\n",
    "else\n",
    "    (echo \"Starting Airflow Webserver\";\n",
    "    airflow db init;\n",
    "    airflow users create \\\n",
    "        --username $_AIRFLOW_WWW_USER_USERNAME \\\n",
    "        --password $_AIRFLOW_WWW_USER_PASSWORD \\\n",
    "        --firstname $_AIRFLOW_WWW_USER_USERNAME \\\n",
    "        --lastname $_AIRFLOW_WWW_USER_USERNAME \\\n",
    "        --role Admin \\\n",
    "        --email admin@example.com;\n",
    "    airflow webserver --port 8080 -D)\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bde93c-cb99-4633-ad0e-b67da88fc808",
   "metadata": {},
   "outputs": [],
   "source": [
    "while not os.path.exists('./airflow/airflow-webserver.pid'):\n",
    "    time.sleep(10)\n",
    "    print(\"Starting Airflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9825d45-e22f-49ba-9b6f-c6b0db2ba6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --bg --out script_out --err script_error\n",
    "FILE=./airflow/airflow-scheduler1.pid\n",
    "if test -f \"$FILE\"; then\n",
    "    echo \"Airflow Scheduler1 is running\"\n",
    "else\n",
    "    airflow scheduler --pid ./airflow/airflow-scheduler1.pid -D\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dab6ac-b87d-4673-8104-6abc63a282b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --bg --out script_out --err script_error\n",
    "FILE=./airflow/airflow-scheduler2.pid\n",
    "if test -f \"$FILE\"; then\n",
    "    echo \"Airflow Scheduler2 is running\"\n",
    "else\n",
    "    airflow scheduler --pid ./airflow/airflow-scheduler2.pid -D\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a4729f-5f2e-4327-b79b-fbd78a2c0ad5",
   "metadata": {},
   "source": [
    "## Create Airflow connections for lakeFS and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b03e8-ae67-4efe-94c6-81718f12224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! airflow connections delete conn_lakefs\n",
    "lakeFSConnectionCommand = 'airflow connections add conn_lakefs --conn-type=http --conn-host=' + lakefsEndPoint + ' --conn-extra=\\'{\"access_key_id\":\"' + lakefsAccessKey + '\",\"secret_access_key\":\"' + lakefsSecretKey + '\"}\\''\n",
    "! $lakeFSConnectionCommand > ./airflow/airflow-connection.txt\n",
    "with open(\"./airflow/airflow-connection.txt\", \"r\") as file:\n",
    "    last_line = file.readlines()[-1]\n",
    "print(last_line)\n",
    "\n",
    "! airflow connections delete conn_spark\n",
    "sparkConnectionCommand = 'airflow connections add conn_spark --conn-type=spark --conn-host=local[*]'\n",
    "! $sparkConnectionCommand > ./airflow/airflow-connection.txt\n",
    "with open(\"./airflow/airflow-connection.txt\", \"r\") as file:\n",
    "    last_line = file.readlines()[-1]\n",
    "print(last_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113d0e32-140a-43c5-8c4d-fe08b8b963eb",
   "metadata": {},
   "source": [
    "## Set Airflow variables which are used by the demo workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1ccd19-3f52-4239-9c3d-90d575a86383",
   "metadata": {},
   "outputs": [],
   "source": [
    "! airflow variables set lakefsAccessKey $lakefsAccessKey\n",
    "! airflow variables set lakefsSecretKey $lakefsSecretKey\n",
    "! airflow variables set lakefsEndPoint $lakefsEndPoint\n",
    "! airflow variables set repo $repo\n",
    "! airflow variables set sourceBranch $sourceBranch\n",
    "! airflow variables set newBranch $newBranch\n",
    "! airflow variables set conn_lakefs 'conn_lakefs'\n",
    "\n",
    "spark_home = os.getenv('SPARK_HOME')\n",
    "! airflow variables set spark_home $spark_home\n",
    "\n",
    "if lakefsEndPoint.startswith('http://host.docker.internal'):\n",
    "    lakefsUIEndPoint = 'http://127.0.0.1:8000'\n",
    "else:\n",
    "    lakefsUIEndPoint = lakefsEndPoint\n",
    "! airflow variables set lakefsUIEndPoint $lakefsUIEndPoint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
