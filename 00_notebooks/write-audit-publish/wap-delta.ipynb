{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8e13cd9",
   "metadata": {},
   "source": [
    "<img src=\"https://docs.delta.io/latest/_static/delta-lake-logo.png\" alt=\"Delta Lake logo\" width=300/>  &nbsp;  &nbsp; &nbsp; <img src=\"../images/logo.svg\" alt=\"lakeFS logo\" width=300/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78797e22",
   "metadata": {},
   "source": [
    "## Write-Audit-Publish (WAP) pattern with Delta Lake and lakeFS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f89a51c",
   "metadata": {},
   "source": [
    "Please see the accompanying blog series for more details: \n",
    "\n",
    "1. [Data Engineering Patterns: Write-Audit-Publish (WAP)](https://lakefs.io/blog/data-engineering-patterns-write-audit-publish)\n",
    "1. [How to Implement Write-Audit-Publish (WAP)](https://lakefs.io/blog/how-to-implement-write-audit-publish)\n",
    "1. [Putting the Write-Audit-Publish Pattern into Practice with lakeFS](https://lakefs.io/blog/write-audit-publish-with-lakefs/)\n",
    "\n",
    "[@rmoff](https://twitter.com/rmoff/) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89718426",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f824b929",
   "metadata": {},
   "source": [
    "## Set up the connection to lakeFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e44584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_client.client import LakeFSClient\n",
    "\n",
    "lakefs_config = lakefs_client.Configuration()\n",
    "lakefs_config.username = 'AKIAIOSFODNN7EXAMPLE'\n",
    "lakefs_config.password = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'\n",
    "lakefs_config.host = 'http://lakefs:8000'\n",
    "\n",
    "lakefs = LakeFSClient(lakefs_config)\n",
    "lakefs_api_client = lakefs_client.ApiClient(lakefs_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0047139f",
   "metadata": {},
   "source": [
    "### Get the first repository present in lakeFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb017136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo=lakefs.repositories.list_repositories().results[0]\n",
    "print(f\"Using lakeFS repository '{repo.id}' with storage namespace {repo.storage_namespace}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7517f0ac",
   "metadata": {},
   "source": [
    "### Define the data storage directory based on the provided namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b647611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir=repo.storage_namespace.replace('s3','s3a')\n",
    "print(f\"Using {data_dir} for data storage\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e4e906a",
   "metadata": {},
   "source": [
    "## Set up Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36742058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"lakeFS / Jupyter\") \\\n",
    "        .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://lakefs:8000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"AKIAIOSFODNN7EXAMPLE\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\") \\\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "spark\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eceeaa9b",
   "metadata": {},
   "source": [
    "## Load test data and write it to the `main` branch as a Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad496308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"inferSchema\",\"true\").option(\"multiline\",\"true\").json(\"/data/nyc_film_permits.json\")\n",
    "\n",
    "permits_file=(f\"{data_dir}/main/nyc/permits\")\n",
    "print(permits_file)\n",
    "df.write.format(\"delta\").mode('overwrite').save(permits_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "545fb202",
   "metadata": {},
   "source": [
    "### Inspect the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4b30a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DESCRIBE EXTENDED delta.`s3a://example/main/nyc/permits`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08f17847",
   "metadata": {},
   "source": [
    "### What does the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6268496",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT borough, count(*) permit_cnt\n",
    "FROM delta.`s3a://example/main/nyc/permits`\n",
    "GROUP BY borough"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bcdcb3c",
   "metadata": {},
   "source": [
    "## Commit the data to the `main` branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0078cc8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api_client = lakefs_client.ApiClient(lakefs_config)\n",
    "\n",
    "from lakefs_client.api import commits_api\n",
    "from lakefs_client.model.commit import Commit\n",
    "from lakefs_client.model.commit_creation import CommitCreation\n",
    "\n",
    "api_instance = commits_api.CommitsApi(api_client)\n",
    "commit_creation = CommitCreation(\n",
    "    message=\"First commit of NYC Permit data\"\n",
    ") \n",
    "\n",
    "\n",
    "api_instance.commit(repo.id, 'main', commit_creation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd3c5e94",
   "metadata": {},
   "source": [
    "# The Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5342229a",
   "metadata": {},
   "source": [
    "lakeFS is based on branches (just like git). Branches are copy-on-write, making them 'cheap' in terms of storage. \n",
    "\n",
    "We're going to create a branch to write data to, audit it, and then merge it back if we're happy with the audit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa39ade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "branch='etl_job_42'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a62f81b",
   "metadata": {},
   "source": [
    "### Create branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecd1769",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lakefs_client.api import branches_api\n",
    "from lakefs_client.model.branch_creation import BranchCreation\n",
    "\n",
    "api_instance = branches_api.BranchesApi(lakefs_api_client)\n",
    "branch_creation = BranchCreation(\n",
    "    name=branch,\n",
    "    source=\"main\",\n",
    ") \n",
    "\n",
    "api_instance.create_branch(repo.id, branch_creation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "886ac446",
   "metadata": {},
   "source": [
    "### Check that we still see the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb40217",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT borough, count(*) permit_cnt\n",
    "FROM delta.`s3a://example/etl_job_42/nyc/permits`\n",
    "GROUP BY borough"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ddc4ff7",
   "metadata": {},
   "source": [
    "# Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291843b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DELETE FROM delta.`s3a://example/etl_job_42/nyc/permits`\n",
    "WHERE borough='Manhattan'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f6c3a04",
   "metadata": {},
   "source": [
    "## Inspecting the staged/unpublished data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c09de33",
   "metadata": {},
   "source": [
    "### Staged/unpublished data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d73e5ba8",
   "metadata": {},
   "source": [
    "#### The changes are reflected in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1329060f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT borough, count(*) permit_cnt\n",
    "FROM delta.`s3a://example/etl_job_42/nyc/permits`\n",
    "GROUP BY borough"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60e26a26",
   "metadata": {},
   "source": [
    "### Published data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e5a4eb7",
   "metadata": {},
   "source": [
    "The data on the `main` branch remains unchanged. We can validate this by running a query against the data, specifying `main` as the branch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61dd829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT borough, count(*) permit_cnt\n",
    "FROM delta.`s3a://example/main/nyc/permits`\n",
    "GROUP BY borough"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a58244fc",
   "metadata": {},
   "source": [
    "# Audit "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "590d8ce7",
   "metadata": {},
   "source": [
    "At the moment the data is written to the audit branch (`etl_job_42`), but not published to `main`. \n",
    "\n",
    "How you audit the data is up to you. The nice thing about the data being staged is that you can do it within the same ETL job, or have another tool do it.\n",
    "\n",
    "Here's a very simple example of doing in Python. We're going to programatically check that only the four expected boroughs remain in the data.\n",
    "\n",
    "First, we define those that are expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d6b827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "expected_boroughs = {\"Queens\", \"Brooklyn\", \"Bronx\", \"Staten Island\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c05b9a5",
   "metadata": {},
   "source": [
    "Then we get a set of the actual boroughs in the staged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77abc608",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distinct_boroughs = spark.read \\\n",
    "                    .format(\"delta\") \\\n",
    "                    .load(\"s3a://example/etl_job_42/nyc/permits\") \\\n",
    "                    .select(\"borough\") \\\n",
    "                    .distinct() \\\n",
    "                    .toLocalIterator()\n",
    "boroughs = {row[0] for row in distinct_boroughs}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81156579",
   "metadata": {},
   "source": [
    "Now we do two checks:\n",
    "\n",
    "1. Compare the length of the expected vs actual set\n",
    "2. Check that the two sets when unioned are still the same length. This is necessary, since the first test isn't sufficient alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91668d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (   (len(boroughs)          != len(expected_boroughs)) \\\n",
    "      or (len(boroughs)          != len(set.union(boroughs, expected_boroughs))) \\\n",
    "      or (len(expected_boroughs) != len(set.union(boroughs, expected_boroughs)))):\n",
    "    raise ValueError(f\"Audit failed, borough set does not match expected boroughs: {boroughs} != {expected_boroughs}\")\n",
    "else:\n",
    "    print(f\"Audit has passed üôåüèª\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d19526bb",
   "metadata": {},
   "source": [
    "# Publish"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6dc07949",
   "metadata": {},
   "source": [
    "Publishing data in lakeFS means merging the audit branch back into `main`, making it available to anyone working with the data in that branch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a637a54",
   "metadata": {},
   "source": [
    "## Commit the data to the audit branch (`etl_job_42`)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01b466db",
   "metadata": {},
   "source": [
    "We can add a commit message, as well as optional metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff1ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lakefs_client.api import commits_api\n",
    "from lakefs_client.model.commit import Commit\n",
    "from lakefs_client.model.commit_creation import CommitCreation\n",
    "\n",
    "api_instance = commits_api.CommitsApi(api_client)\n",
    "commit_creation = CommitCreation(\n",
    "    message=\"Remove data for Manhattan from permits dataset\",\n",
    "    metadata={\n",
    "        \"etl job name\": \"etl_job_42\",\n",
    "        \"author\": \"rmoff\",\n",
    "    }\n",
    ") \n",
    "\n",
    "api_instance.commit(repo.id, branch, commit_creation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bdce995",
   "metadata": {},
   "source": [
    "## Merge the branch back into `main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0375e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.refs.merge_into_branch(repository=repo.id, source_ref='etl_job_42', destination_branch='main')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d38524dd",
   "metadata": {},
   "source": [
    "## Inspecting the published data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ea790c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT borough, count(*) permit_cnt\n",
    "FROM delta.`s3a://example/main/nyc/permits`\n",
    "GROUP BY borough"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7482b2db",
   "metadata": {},
   "source": [
    "# Where Next?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "814d43a5",
   "metadata": {},
   "source": [
    "* For more information about write-audit-publish see [this talk from Michelle Winters](https://www.youtube.com/watch?v=fXHdeBnpXrg&t=1001s) and [this talk from Sam Redai](https://www.dremio.com/wp-content/uploads/2022/05/Sam-Redai-The-Write-Audit-Publish-Pattern-via-Apache-Iceberg.pdf).\n",
    "* To try out lakeFS check out the [hands-on Quickstart](https://docs.lakefs.io/quickstart/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
