{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d8a1d2b",
   "metadata": {},
   "source": [
    "<img src=\"../images/logo.svg\" alt=\"lakeFS logo\" width=300/> \n",
    "\n",
    "# ML Experimentation 01 (Dogs)\n",
    "\n",
    "In this tutorial, you will learn how to version your ML training data, model artifacts, metrics and  your training code together with lakeFS.\n",
    "\n",
    "We will be using a subset of the [Stanford-Dogs-Dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/) (aka ImageNetDogs) for the image classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f522dc6",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8633c2c9",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "**_If you're not using the provided lakeFS server and MinIO storage then change these values to match your environment_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3424646",
   "metadata": {},
   "source": [
    "### lakeFS endpoint and credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f0adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefsEndPoint = 'http://lakefs:8000' # e.g. 'https://username.aws_region_name.lakefscloud.io' \n",
    "lakefsAccessKey = 'AKIAIOSFODNN7EXAMPLE'\n",
    "lakefsSecretKey = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8663ef6a",
   "metadata": {},
   "source": [
    "### Object Storage\n",
    "\n",
    "The storageNamespace in lakeFS needs to be unique per repository. \n",
    "\n",
    "The value given here will be combined with the repo name to create the storage namespace used. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a835b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storageNamespace = 's3://example' # e.g. \"s3://bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff931add-492f-4f67-a08d-0fd8308d5b8d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9805f763",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**(you shouldn't need to change anything in this section, just run it)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eacb29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_name = \"lakefs-minio-demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa5e22d",
   "metadata": {},
   "source": [
    "### Create lakeFSClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a553232",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_client.models import *\n",
    "from lakefs_client.client import LakeFSClient\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = lakefsAccessKey\n",
    "configuration.password = lakefsSecretKey\n",
    "configuration.host = lakefsEndPoint\n",
    "\n",
    "lakefs = LakeFSClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3949b16",
   "metadata": {},
   "source": [
    "#### Verify lakeFS credentials by getting lakeFS version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2270980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Verifying lakeFS credentialsâ€¦\")\n",
    "try:\n",
    "    v=lakefs.config.get_lake_fs_version()\n",
    "except:\n",
    "    print(\"ðŸ›‘ failed to get lakeFS version\")\n",
    "else:\n",
    "    print(f\"â€¦âœ…lakeFS credentials verified\\n\\nâ„¹ï¸lakeFS version {v.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828d4dc9",
   "metadata": {},
   "source": [
    "### Define lakeFS Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97359c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lakefs_client.exceptions import NotFoundException\n",
    "\n",
    "try:\n",
    "    repo=lakefs.repositories.get_repository(repo_name)\n",
    "    print(f\"Found existing repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "except NotFoundException as f:\n",
    "    print(f\"Repository {repo_name} does not exist, so going to try and create it now.\")\n",
    "    try:\n",
    "        repo=lakefs.repositories.create_repository(repository_creation=RepositoryCreation(name=repo_name,\n",
    "                                                                                                storage_namespace=f\"{storageNamespace}/{repo_name}\"))\n",
    "        print(f\"Created new repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "    except lakefs_client.ApiException as e:\n",
    "        print(f\"Error creating repo {repo_name}. Error is {e}\")\n",
    "        os._exit(00)\n",
    "except lakefs_client.ApiException as e:\n",
    "    print(f\"Error getting repo {repo_name}: {e}\")\n",
    "    os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919654c2",
   "metadata": {},
   "source": [
    "### Set up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c376fac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"lakeFS / Jupyter\") \\\n",
    "        .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", lakefsEndPoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", lakefsAccessKey) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", lakefsSecretKey) \\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbffb17f",
   "metadata": {},
   "source": [
    "### Install libraries\n",
    "\n",
    "_`boto3` is pinned because of `cannot import name 'DEPRECATED_SERVICE_NAMES' from 'botocore.docs'` error_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3731c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install opencv-python tensorflow nbimporter s3fs boto3==1.26.90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9285a279",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad5727",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import s3fs\n",
    "import joblib\n",
    "import tempfile\n",
    "from io import BytesIO\n",
    "import nbimporter\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c266d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils_ml_reproducibility.ml_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9996d1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import date, time, datetime\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "#from keras.import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7b080a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0ad76",
   "metadata": {},
   "source": [
    "### Configure boto3 client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3aeda5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3',\n",
    "    endpoint_url=lakefsEndPoint,\n",
    "    aws_access_key_id=lakefsAccessKey,\n",
    "    aws_secret_access_key=lakefsSecretKey)\n",
    "\n",
    "s3_resource = boto3.resource('s3',\n",
    "    endpoint_url=lakefsEndPoint,\n",
    "    aws_access_key_id=lakefsAccessKey,\n",
    "    aws_secret_access_key=lakefsSecretKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549d8e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem(anon=False,\n",
    "                      key=lakefsAccessKey,\n",
    "                      secret=lakefsSecretKey,\n",
    "                      client_kwargs={'endpoint_url': lakefsEndPoint})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f7996-326c-427a-aa1f-e762f368eae8",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6351dc-8dbb-4f1f-97db-f850ed9eb27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_from_s3(bucket, key):\n",
    "    \n",
    "    bucket = s3_resource.Bucket(bucket)\n",
    "    file_stream = BytesIO()\n",
    "    bucket.Object(key).download_fileobj(file_stream)\n",
    "    np_1d_array = np.frombuffer(file_stream.getbuffer(), dtype=\"uint8\")\n",
    "    img = cv2.imdecode(np_1d_array, cv2.IMREAD_COLOR).copy()\n",
    "    \n",
    "    return resize_img(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ada404-5a7b-472c-b029-3d39ea8e7f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_list_from_s3(bucket, key, delimiter, n_cats):\n",
    "    \n",
    "    list_resp = s3_client.list_objects_v2(Bucket=bucket, \n",
    "                                          Prefix=key+\"/\",\n",
    "                                         Delimiter=delimiter)\n",
    "    print(\"List_resp\", list_resp)\n",
    "    \n",
    "    category_list = [ x['Prefix'] for x in list_resp['CommonPrefixes'][:n_cats]]\n",
    "    print(category_list)\n",
    "    \n",
    "    return category_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f7e8fc-6c8f-4e15-8949-292d67e041a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_and_labels(bucket, category_list, n_images):\n",
    "\n",
    "    img_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for index, category in enumerate(category_list):\n",
    "        # breed = category.split(\"/\")[-2]\n",
    "        list_resp = s3_client.list_objects_v2(Bucket=bucket, Prefix=category)\n",
    "\n",
    "        for c in list_resp['Contents'][:n_images]:\n",
    "            key = c['Key']\n",
    "            img = get_img_from_s3(bucket, key)\n",
    "            label = index\n",
    "            \n",
    "            img_list.append(img)\n",
    "            labels_list.append(label)\n",
    "    \n",
    "    images = np.array(img_list)\n",
    "    labels = np.array(labels_list)\n",
    "\n",
    "    # print(\"Images shape = \",images.shape,\"\\nLabels shape = \",labels.shape)\n",
    "    # print(type(images),type(labels))\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70cc139-7615-47a0-98d9-4dfcec4fdb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(params):\n",
    "    \n",
    "    category_list = get_category_list_from_s3(bucket=params['repo_name'],\n",
    "                                         key=params['image_path'],\n",
    "                                         delimiter=params['delimiter'],\n",
    "                                         n_cats=params['n_cats']\n",
    "                                         )\n",
    "    \n",
    "\n",
    "    images, labels = get_images_and_labels(bucket=repo_name,\n",
    "                                             category_list=category_list,\n",
    "                                             n_images=params['n_images'])\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ccc2b-5faf-41d0-b904-645348adff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load(model_name, bucket_name, key):\n",
    "    \n",
    "    key = f\"{key}/{model_name}\"\n",
    "    \n",
    "    # READ\n",
    "    with tempfile.TemporaryFile() as fp:\n",
    "        s3_client.download_fileobj(Fileobj=fp, Bucket=bucket_name, Key=key)\n",
    "        fp.seek(0)\n",
    "        model = joblib.load(fp)\n",
    "\n",
    "    # DELETE\n",
    "    # s3_client.delete_object(Bucket=bucket_name, Key=key)\n",
    "    \n",
    "    print(type(model))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d898816-8a84-4f5e-abac-1f4fa102a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_save(model, model_name, bucket_name, key):\n",
    "    \n",
    "    joblib.dump(model, model_name)\n",
    "    \n",
    "    key = f\"{key}/{model_name}\"\n",
    "    print(model_name, bucket_name, key)\n",
    "\n",
    "    # WRITE\n",
    "    with tempfile.TemporaryFile() as fp:\n",
    "        joblib.dump(model, fp)\n",
    "        fp.seek(0)\n",
    "        s3_client.put_object(Body=fp.read(), Bucket=bucket_name, Key=key)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683e90f-0380-4d84-95cc-44251749e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics(metrics, bucket_name, key):\n",
    "    \n",
    "    data = [(str(metrics['loss']), str(metrics['accuracy']))]\n",
    "\n",
    "    schema = StructType([ \\\n",
    "        StructField(\"loss\",StringType(),True), \\\n",
    "        StructField(\"accuracy\",StringType(),True) \\\n",
    "      ])\n",
    " \n",
    "    df = spark.createDataFrame(data=data,schema=schema)\n",
    "    df.printSchema()\n",
    "    df.show(truncate=False)\n",
    "    \n",
    "    path = f\"s3a://{bucket_name}/{key}\"\n",
    "    df.write.json(path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e9978-6e3f-4ada-a0e1-a8096b4c6be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics(bucket_name, key):\n",
    "    \n",
    "    path = f\"s3a://{bucket_name}/{key}\"\n",
    "    \n",
    "    df = spark.read.json(path)\n",
    "    metrics = df.collect()[0]\n",
    "    loss = metrics['loss']\n",
    "    accuracy = metrics['accuracy']\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37562fc7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212abdd",
   "metadata": {},
   "source": [
    "# Main Tutorial starts here ðŸš¦ ðŸ‘‡ðŸ»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7785469-593f-4cfb-930f-7b55f4e9c19e",
   "metadata": {},
   "source": [
    "## Zero clone import from MinIo bucket to the lakeFS Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6282396-d56c-4ec5-a1f5-9e81fd36801b",
   "metadata": {},
   "source": [
    "### Create the ingestion branch to import into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997200c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ingestBranch = \"ingest\"        # This is the branch we will load data into\n",
    "exp1_branch = \"experiment-1\"   # Each experiment will run on a seperate branch\n",
    "exp2_branch = \"experiment-2\"\n",
    "\n",
    "prod_branch = \"main\"           # The best result will be promoted to production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f100f836-b235-4dc0-b69d-25a18852b79c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.create_branch(\n",
    "    repository=repo.id,\n",
    "    branch_creation=BranchCreation(\n",
    "        name=ingestBranch,\n",
    "        source=prod_branch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd4604-a8f8-4520-9365-e2b2923ccbd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Sources and Destinations\n",
    "importSource1 = \"s3://sample-data/stanfordogsdataset/Images\"     # Images library\n",
    "importSource2 = \"s3://sample-data/stanfordogsdataset/Annotation\" # Annotation library\n",
    "\n",
    "importDestination = \"raw/\" # Path to import into within the lakeFS repository\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88def362-4d78-403a-b91f-cc5996977b2c",
   "metadata": {},
   "source": [
    "### Execute the import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6afbef-c0bb-49e4-bd9d-1eeb9d4e3305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Start Import\n",
    "import_api = lakefs.__dict__[\"import\"]\n",
    "commit = CommitCreation(message=\"import objects\", metadata={\"key\": \"value\"})\n",
    "paths=[\n",
    "    ImportLocation(type=\"common_prefix\", path=importSource1, destination=importDestination),\n",
    "    ImportLocation(type=\"common_prefix\", path=importSource2, destination=importDestination)\n",
    "]\n",
    "import_creation = ImportCreation(paths=paths, commit=commit)\n",
    "create_resp = import_api.import_start(repo.id, ingestBranch, import_creation)\n",
    "\n",
    "# Wait for import to finish\n",
    "while True:\n",
    "    status_resp = import_api.import_status(repo.id, ingestBranch, create_resp.id)\n",
    "    print(status_resp)\n",
    "    if hasattr(status_resp, \"Error in import\"):\n",
    "        raise Exception(status_resp.err)\n",
    "    if status_resp.completed:\n",
    "        print(\"Import completed Successfully. Data imported into branch:\", ingestBranch)\n",
    "        break\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2521f2",
   "metadata": {},
   "source": [
    "## Experiments storage locations configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b6c394",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = f\"s3a://{repo_name}\"\n",
    "\n",
    "images_path = \"Images\"\n",
    "annotations = \"Annotations\"\n",
    "\n",
    "raw_path = \"raw\"\n",
    "config_path = \"config\"\n",
    "artifact_path = \"artifacts\"\n",
    "metrics_path = \"metrics\"\n",
    "training_code_path = \"src\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f96762",
   "metadata": {},
   "source": [
    "# Experimentation Begins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811c8da2",
   "metadata": {},
   "source": [
    "## Experiment #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84933890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params_exp1 ={\n",
    "    'repo_name': repo_name,\n",
    "    'branch': exp1_branch,\n",
    "    'image_path': f\"{exp1_branch}/{raw_path}/{images_path}\",\n",
    "    'artifacts_path': f\"{exp1_branch}/{artifact_path}\",\n",
    "    'metrics_path': f\"{exp1_branch}/{metrics_path}\",\n",
    "    'config_path': f\"{exp1_branch}/{config_path}\",\n",
    "    'model_name': \"model.pkl\",\n",
    "    'delimiter': \"/\",\n",
    "    'n_cats': 3,\n",
    "    'n_images': 250,\n",
    "    'is_shuffle':True,\n",
    "    'is_normalize': False,\n",
    "    'epochs': 50,\n",
    "    'train_test_split_ratio': 0.2,\n",
    "    'optimizer': \"adagrad\",\n",
    "    'loss': \"sparse_categorical_crossentropy\",\n",
    "    'metrics': [\"accuracy\"]\n",
    "}\n",
    "params = params_exp1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041f28aa",
   "metadata": {},
   "source": [
    "### Set up lakeFS for experiment #1\n",
    "\n",
    "#### Create a new branch: `experiment-1` from the ingest branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd15e33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.list_branches(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e357713",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.create_branch(repository=repo_name, \n",
    "                              branch_creation=BranchCreation(name=exp1_branch, \n",
    "                                                                    source=ingestBranch)\n",
    "                             )\n",
    "lakefs.branches.list_branches(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f8c82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('config.json', 'w') as fp:\n",
    "    json.dump(params, fp)\n",
    "    \n",
    "with open(f'./config.json', 'rb') as f:\n",
    "    lakefs.objects.upload_object(repository=repo_name, \n",
    "                                 branch=exp1_branch, \n",
    "                                 path=f\"{config_path}/config.json\", \n",
    "                                 content=f\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aaf934",
   "metadata": {},
   "source": [
    "#### Load training data from lakeFS. \n",
    "#### Generate images and labels for training and Commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ee64f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images, labels = load_training_data(params)\n",
    "print(\"Loading training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af08282d",
   "metadata": {},
   "source": [
    "#### Train the model. \n",
    "#### Upload model metrics to lakeFS and commit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851a84ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1, metrics1 = ml_pipeline(params, images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb92fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_metrics(metrics1, repo_name, params['metrics_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e715f3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params['loss'], params['accuracy'] = load_metrics(repo_name, params['metrics_path'])\n",
    "pprint.pprint(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5881d38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.diff_branch(repository=repo_name, \n",
    "                            branch=exp1_branch).results\n",
    "\n",
    "commit_meta_params = {}\n",
    "for k,v in params.items():\n",
    "    commit_meta_params[k]=str(v)\n",
    "\n",
    "lakefs.commits.commit(repository=repo_name,\n",
    "                      branch=exp1_branch,\n",
    "                      commit_creation=CommitCreation(\n",
    "                          message=f\"Saving model metrics to {exp1_branch}\",\n",
    "                          metadata=commit_meta_params)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502a9d74",
   "metadata": {},
   "source": [
    "#### Upload model artifacts to lakeFS and commit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b838cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_save(model1, \n",
    "           params['model_name'], \n",
    "           params['repo_name'], \n",
    "           params['artifacts_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1175b09a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.diff_branch(repository=repo_name, \n",
    "                            branch=exp1_branch).results\n",
    "\n",
    "commit_meta_params = {}\n",
    "for k,v in params.items():\n",
    "    commit_meta_params[k]=str(v)\n",
    "print(commit_meta_params)\n",
    "\n",
    "lakefs.commits.commit(repository=repo_name,\n",
    "                      branch=exp1_branch,\n",
    "                      commit_creation=CommitCreation(\n",
    "                          message=f\"Saving model artifacts to {exp1_branch}\",\n",
    "                          metadata=commit_meta_params)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4301dc20",
   "metadata": {},
   "source": [
    "#### Load the pickle file from lakeFS, and run predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611dc999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1_reloaded = model_load(params['model_name'], \n",
    "           params['repo_name'], \n",
    "           params['artifacts_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd72477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = split_train_test(images, labels, params['train_test_split_ratio'])\n",
    "pred = model1_reloaded.predict(x_test)\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d5f98d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (19 , 10))\n",
    "n = 0 \n",
    "\n",
    "for i in range(9):\n",
    "    n += 1 \n",
    "    r = np.random.randint( 0, x_test.shape[0], 1)\n",
    "    \n",
    "    plt.subplot(3, 3, n)\n",
    "    plt.subplots_adjust(hspace = 0.3, wspace = 0.3)\n",
    "    \n",
    "    plt.imshow(x_test[r[0]])\n",
    "    plt.title('Actual = {}, Predicted = {}'.format(y_test[r[0]] , y_test[r[0]]*pred[r[0]][y_test[r[0]]]) )\n",
    "    plt.xticks([]) , plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7d91b6",
   "metadata": {},
   "source": [
    "## Experiment #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6211d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params_exp2 ={\n",
    "    'repo_name': repo_name,\n",
    "    'branch': exp2_branch,\n",
    "    'image_path': f\"{exp2_branch}/{raw_path}/{images_path}\",\n",
    "    'artifacts_path': f\"{exp2_branch}/{artifact_path}\",\n",
    "    'metrics_path': f\"{exp2_branch}/{metrics_path}\",\n",
    "    'config_path': f\"{exp2_branch}/{config_path}\",\n",
    "    'model_name': \"model.pkl\",\n",
    "    'delimiter': \"/\",\n",
    "    'n_cats': 3,\n",
    "    'n_images': 300,\n",
    "    'is_shuffle': True,\n",
    "    'is_normalize': True,\n",
    "    'epochs': 10,\n",
    "    'train_test_split_ratio': 0.15,\n",
    "    'optimizer': \"adam\",\n",
    "    'loss': \"sparse_categorical_crossentropy\",\n",
    "    'metrics': [\"accuracy\"]\n",
    "}\n",
    "params = params_exp2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3989e46",
   "metadata": {},
   "source": [
    "### Set up lakeFS for experiment #2\n",
    "\n",
    "1. Create a new branch: `experiment-2` from the ingest branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6202158",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.list_branches(repo_name)\n",
    "\n",
    "lakefs.branches.create_branch(repository=repo_name, \n",
    "                              branch_creation=BranchCreation(name=exp2_branch, \n",
    "                                                                    source=ingestBranch)\n",
    "                             )\n",
    "\n",
    "lakefs.branches.list_branches(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfdef56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('config.json', 'w') as fp:\n",
    "    json.dump(params, fp)\n",
    "    \n",
    "with open(f'./config.json', 'rb') as f:\n",
    "    lakefs.objects.upload_object(repository=repo_name, \n",
    "                                 branch=exp2_branch, \n",
    "                                 path=f\"{config_path}/config.json\", \n",
    "                                 content=f\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81022681",
   "metadata": {},
   "source": [
    "#### Load training data from lakeFS. \n",
    "#### Generate images and labels for training and Commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f3901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images, labels = load_training_data(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d5b5d",
   "metadata": {},
   "source": [
    "#### Train the model. \n",
    "#### Upload model metrics to lakeFS and commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4e18b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2, metrics2 = ml_pipeline(params, images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392eb6b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_metrics(metrics2, repo_name, params['metrics_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a62439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params['loss'], params['accuracy'] = load_metrics(repo_name, params['metrics_path'])\n",
    "pprint.pprint(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c743e85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.diff_branch(repository=repo_name, \n",
    "                            branch=exp2_branch).results\n",
    "\n",
    "commit_meta_params = {}\n",
    "for k,v in params.items():\n",
    "    commit_meta_params[k]=str(v)\n",
    "pprint.pprint(commit_meta_params)\n",
    "\n",
    "lakefs.commits.commit(repository=repo_name,\n",
    "                      branch=exp2_branch,\n",
    "                      commit_creation=CommitCreation(\n",
    "                          message=f\"Saving model metrics to {exp2_branch}\",\n",
    "                          metadata=commit_meta_params)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e983b4",
   "metadata": {},
   "source": [
    "#### Upload model artifacts to lakeFS and commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c0477b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_save(model2, \n",
    "           params['model_name'], \n",
    "           params['repo_name'], \n",
    "           params['artifacts_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445f4d97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.diff_branch(repository=repo_name, \n",
    "                            branch=exp2_branch).results\n",
    "\n",
    "commit_meta_params = {}\n",
    "for k,v in params.items():\n",
    "    commit_meta_params[k]=str(v)\n",
    "pprint.pprint(commit_meta_params)\n",
    "\n",
    "lakefs.commits.commit(repository=repo_name,\n",
    "                      branch=exp2_branch,\n",
    "                      commit_creation=CommitCreation(\n",
    "                          message=f\"Saving model artifacts to {exp2_branch}\",\n",
    "                          metadata=commit_meta_params)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e262d9e6",
   "metadata": {},
   "source": [
    "#### Load the pickle file from lakeFS, and run predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855af9c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2_reloaded = model_load(params['model_name'], \n",
    "           params['repo_name'], \n",
    "           params['artifacts_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db225f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = split_train_test(images, labels, params['train_test_split_ratio'])\n",
    "pred = model2_reloaded.predict(x_test)\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2146e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (19 , 10))\n",
    "n = 0 \n",
    "\n",
    "for i in range(9):\n",
    "    n += 1 \n",
    "    r = np.random.randint( 0, x_test.shape[0], 1)\n",
    "    \n",
    "    plt.subplot(3, 3, n)\n",
    "    plt.subplots_adjust(hspace = 0.3, wspace = 0.3)\n",
    "    \n",
    "    plt.imshow(x_test[r[0]])\n",
    "    plt.title('Actual = {}, Predicted = {}'.format(y_test[r[0]] , y_test[r[0]]*pred[r[0]][y_test[r[0]]]) )\n",
    "    plt.xticks([]) , plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47198a92",
   "metadata": {},
   "source": [
    "### Compare in both branches. Merge the winning model to Prod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e7fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "win_branch = exp2_branch\n",
    "if metrics1['accuracy']> metrics2['accuracy']:\n",
    "    win_branch = exp1_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c824b80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "win_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b02637c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.refs.merge_into_branch(repository=repo_name, \n",
    "                              source_ref=win_branch, \n",
    "                              destination_branch=prod_branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f5674e",
   "metadata": {},
   "source": [
    "## The winning experiment is now promoted into production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
