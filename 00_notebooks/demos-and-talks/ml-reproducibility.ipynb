{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d8a1d2b",
   "metadata": {},
   "source": [
    "<img src=\"../images/logo.svg\" alt=\"lakeFS logo\" width=300/> \n",
    "\n",
    "# ML Experimentation 01 (Dogs)\n",
    "\n",
    "In this tutorial, you will learn how to version your ML training data, model artifacts, metrics and  your training code together with lakeFS.\n",
    "\n",
    "We will be using [Stanford-Dogs-Dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/) (aka ImageNetDogs) for the image classification "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cce2c3c",
   "metadata": {},
   "source": [
    "_ðŸš§ This notebook may have existing environment or data requirements; it's included here so that you can see the contents and be inspired by itâ€”but it may not run properly.ðŸš§_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f522dc6",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7543161f",
   "metadata": {},
   "source": [
    "## Before you start!\n",
    "\n",
    "Download the [Stanford-Dogs-Dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/) to your MinIO bucket. We will be importing this data into a lakeFS repository and use it for ML model training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "995694bf",
   "metadata": {},
   "source": [
    "1. Create a new MinIO bucket for lakeFS repository\n",
    "2. Create a lakeFS repository (ml-demo)\n",
    "3. Import dataset into the lakeFS repo (branch `_main_imported`)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8633c2c9",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "**_If you're not using the provided lakeFS server and MinIO storage then change these values to match your environment_**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3424646",
   "metadata": {},
   "source": [
    "### lakeFS endpoint and credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f0adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefsEndPoint = 'http://lakefs:8000' # e.g. 'https://username.aws_region_name.lakefscloud.io' \n",
    "lakefsAccessKey = 'AKIAIOSFODNN7EXAMPLE'\n",
    "lakefsSecretKey = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8663ef6a",
   "metadata": {},
   "source": [
    "### Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a835b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "storageNamespace = 's3://example' # e.g. \"s3://bucket\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77635c43",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9805f763",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**(you shouldn't need to change anything in this section, just run it)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eacb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"ml-demo\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fa5e22d",
   "metadata": {},
   "source": [
    "### Create lakeFSClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a553232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_lakefs.import *\n",
    "from lakefs_lakefs.client import LakeFSClient\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = lakefsAccessKey\n",
    "configuration.password = lakefsSecretKey\n",
    "configuration.host = lakefsEndPoint\n",
    "\n",
    "lakefs = LakeFSClient(configuration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3949b16",
   "metadata": {},
   "source": [
    "#### Verify lakeFS credentials by getting lakeFS version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2270980",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verifying lakeFS credentialsâ€¦\")\n",
    "try:\n",
    "    v=lakefs.config.get_lake_fs_version()\n",
    "except:\n",
    "    print(\"ðŸ›‘ failed to get lakeFS version\")\n",
    "else:\n",
    "    print(f\"â€¦âœ…lakeFS credentials verified\\n\\nâ„¹ï¸lakeFS version {v.version}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "828d4dc9",
   "metadata": {},
   "source": [
    "### Define lakeFS Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97359c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakefs_client.exceptions import NotFoundException\n",
    "\n",
    "try:\n",
    "    repo=lakefs.repositories.get_repository(repo_name)\n",
    "    print(f\"Found existing repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "except NotFoundException as f:\n",
    "    print(f\"Repository {repo_name} does not exist, so going to try and create it now.\")\n",
    "    try:\n",
    "        repo=lakefs.repositories.create_repository(repository_creation=RepositoryCreation(name=repo_name,\n",
    "                                                                                                storage_namespace=f\"{storageNamespace}/{repo_name}\"))\n",
    "        print(f\"Created new repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "    except lakefs_client.ApiException as e:\n",
    "        print(f\"Error creating repo {repo_name}. Error is {e}\")\n",
    "        os._exit(00)\n",
    "except lakefs_client.ApiException as e:\n",
    "    print(f\"Error getting repo {repo_name}: {e}\")\n",
    "    os._exit(00)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "919654c2",
   "metadata": {},
   "source": [
    "### Set up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c376fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"lakeFS / Jupyter\") \\\n",
    "        .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", lakefsEndPoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", lakefsAccessKey) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", lakefsSecretKey) \\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbffb17f",
   "metadata": {},
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install opencv-python tensorflow nbimporter s3fs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9285a279",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad5727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import s3fs\n",
    "import joblib\n",
    "import tempfile\n",
    "from io import BytesIO\n",
    "import nbimporter\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c266d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_reproducibility.ml_utils import *\n",
    "from ml_reproducibility.file_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9996d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, time, datetime\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7b080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55e0ad76",
   "metadata": {},
   "source": [
    "### Configure boto3 client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3aeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3',\n",
    "    endpoint_url='http://lakefs:8000',\n",
    "    aws_access_key_id=lakefsAccessKey,\n",
    "    aws_secret_access_key=lakefsSecretKey)\n",
    "\n",
    "s3_resource = boto3.resource('s3',\n",
    "    endpoint_url='http://lakefs:8000',\n",
    "    aws_access_key_id=lakefsAccessKey,\n",
    "    aws_secret_access_key=lakefsSecretKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549d8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem(anon=False,\n",
    "                      key=lakefsAccessKey,\n",
    "                      secret=lakefsSecretKey,\n",
    "                      client_kwargs={'endpoint_url': lakefsEndPoint})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37562fc7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c212abdd",
   "metadata": {},
   "source": [
    "# Main Tutorial starts here ðŸš¦ ðŸ‘‡ðŸ»"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe2521f2",
   "metadata": {},
   "source": [
    "## Experiment Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997200c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_branch = \"_main_imported\"\n",
    "exp1_branch = \"experiment-1\"\n",
    "exp2_branch = \"experiment-2\"\n",
    "\n",
    "prod_branch = \"main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b6c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"s3a://{repo_name}\"\n",
    "\n",
    "images_path = \"dogs_dataset_/images/Images\"\n",
    "annotations = \"dogs_dataset_/annotations/Annotations\"\n",
    "\n",
    "raw_path = \"raw\"\n",
    "processed_path = \"processed\"\n",
    "config_path = \"config\"\n",
    "artifact_path = \"artifacts\"\n",
    "metrics_path = \"metrics\"\n",
    "training_code_path = \"src\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4f96762",
   "metadata": {},
   "source": [
    "# Experimentation Begins"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "811c8da2",
   "metadata": {},
   "source": [
    "## Experiment #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84933890",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_exp1 ={\n",
    "    'repo_name': repo_name,\n",
    "    'branch': exp1_branch,\n",
    "    'image_path': f\"{exp1_branch}/{raw_path}/{images_path}\",\n",
    "    'artifacts_path': f\"{exp1_branch}/{artifact_path}\",\n",
    "    'metrics_path': f\"{exp1_branch}/{metrics_path}\",\n",
    "    'config_path': f\"{exp1_branch}/{config_path}\",\n",
    "    'model_name': \"model.pkl\",\n",
    "    'delimiter': \"/\",\n",
    "    'n_cats': 3,\n",
    "    'n_images': 100,\n",
    "    'is_shuffle':True,\n",
    "    'is_normalize': False,\n",
    "    'epochs': 200,\n",
    "    'train_test_split_ratio': 0.2,\n",
    "    'optimizer': \"adam\",\n",
    "    'loss': \"sparse_categorical_crossentropy\",\n",
    "    'metrics': [\"accuracy\"]\n",
    "}\n",
    "params = params_exp1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "041f28aa",
   "metadata": {},
   "source": [
    "### Set up lakeFS for experiment #1\n",
    "\n",
    "#### Create a new branch: `experiment-1` from `_main_exported`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd15e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefs.branches.list_branches(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e357713",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefs.branches.create_branch(repository=repo_name, \n",
    "                              branch_creation=BranchCreation(name=exp1_branch, \n",
    "                                                                    source=ingest_branch)\n",
    "                             )\n",
    "lakefs.branches.list_branches(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f8c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json', 'w') as fp:\n",
    "    json.dump(params, fp)\n",
    "    \n",
    "with open(f'./config.json', 'rb') as f:\n",
    "    lakefs.objects.upload_object(repository=repo_name, \n",
    "                                 branch=exp1_branch, \n",
    "                                 path=f\"{config_path}/config.json\", \n",
    "                                 content=f\n",
    "                                )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1aaf934",
   "metadata": {},
   "source": [
    "#### Load training data from lakeFS. \n",
    "#### Generate images and labels for training and Commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ee64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_training_data(params)\n",
    "print(\"Loading training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246e0e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Commit the training data after preprocessing under /processed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af08282d",
   "metadata": {},
   "source": [
    "#### Train the model. \n",
    "#### Upload model metrics to lakeFS and commit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851a84ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1, metrics1 = ml_pipeline(params, images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb92fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_metrics(metrics1, repo_name, params['metrics_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e715f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['loss'], params['accuracy'] = load_metrics(repo_name, params['metrics_path'])\n",
    "pprint.pprint(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5881d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefs.branches.diff_branch(repository=repo_name, \n",
    "                            branch=exp1_branch).results\n",
    "\n",
    "commit_meta_params = {}\n",
    "for k,v in params.items():\n",
    "    commit_meta_params[k]=str(v)\n",
    "\n",
    "lakefs.commits.commit(repository=repo_name,\n",
    "                      branch=exp1_branch,\n",
    "                      commit_creation=CommitCreation(\n",
    "                          message=f\"Saving model metrics to {exp1_branch}\",\n",
    "                          metadata=commit_meta_params)\n",
    "                     )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "502a9d74",
   "metadata": {},
   "source": [
    "#### Upload model artifacts to lakeFS and commit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b838cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save(model1, \n",
    "           params['model_name'], \n",
    "           params['repo_name'], \n",
    "           params['artifacts_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1175b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefs.branches.diff_branch(repository=repo_name, \n",
    "                            branch=exp1_branch).results\n",
    "\n",
    "commit_meta_params = {}\n",
    "for k,v in params.items():\n",
    "    commit_meta_params[k]=str(v)\n",
    "print(commit_meta_params)\n",
    "\n",
    "lakefs.commits.commit(repository=repo_name,\n",
    "                      branch=exp1_branch,\n",
    "                      commit_creation=CommitCreation(\n",
    "                          message=f\"Saving model artifacts to {exp1_branch}\",\n",
    "                          metadata=commit_meta_params)\n",
    "                     )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4301dc20",
   "metadata": {},
   "source": [
    "#### Load the pickle file from lakeFS, and run predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611dc999",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_reloaded = model_load(params['model_name'], \n",
    "           params['repo_name'], \n",
    "           params['artifacts_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd72477",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = split_train_test(images, labels, params['train_test_split_ratio'])\n",
    "pred = model1_reloaded.predict(x_test)\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d5f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (19 , 10))\n",
    "n = 0 \n",
    "\n",
    "for i in range(9):\n",
    "    n += 1 \n",
    "    r = np.random.randint( 0, x_test.shape[0], 1)\n",
    "    \n",
    "    plt.subplot(3, 3, n)\n",
    "    plt.subplots_adjust(hspace = 0.3, wspace = 0.3)\n",
    "    \n",
    "    plt.imshow(x_test[r[0]])\n",
    "    plt.title('Actual = {}, Predicted = {}'.format(y_test[r[0]] , y_test[r[0]]*pred[r[0]][y_test[r[0]]]) )\n",
    "    plt.xticks([]) , plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c7d91b6",
   "metadata": {},
   "source": [
    "## Experiment #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6211d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_exp2 ={\n",
    "    'repo_name': repo_name,\n",
    "    'branch': exp2_branch,\n",
    "    'image_path': f\"{exp2_branch}/{raw_path}/{images_path}\",\n",
    "    'artifacts_path': f\"{exp2_branch}/{artifact_path}\",\n",
    "    'metrics_path': f\"{exp2_branch}/{metrics_path}\",\n",
    "    'config_path': f\"{exp2_branch}/{config_path}\",\n",
    "    'model_name': \"model.pkl\",\n",
    "    'delimiter': \"/\",\n",
    "    'n_cats': 3,\n",
    "    'n_images': 50,\n",
    "    'is_shuffle': True,\n",
    "    'is_normalize': True,\n",
    "    'epochs': 10,\n",
    "    'train_test_split_ratio': 0.15,\n",
    "    'optimizer': \"adagrad\",\n",
    "    'loss': \"sparse_categorical_crossentropy\",\n",
    "    'metrics': [\"accuracy\"]\n",
    "}\n",
    "params = params_exp2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3989e46",
   "metadata": {},
   "source": [
    "### Set up lakeFS for experiment #2\n",
    "\n",
    "1. Create a new branch: `experiment-2` from `_main_exported`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6202158",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefs.branches.list_branches(repo_name)\n",
    "\n",
    "lakefs.branches.create_branch(repository=repo_name, \n",
    "                              branch_creation=BranchCreation(name=exp2_branch, \n",
    "                                                                    source=ingest_branch)\n",
    "                             )\n",
    "\n",
    "lakefs.branches.list_branches(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfdef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json', 'w') as fp:\n",
    "    json.dump(params, fp)\n",
    "    \n",
    "with open(f'./config.json', 'rb') as f:\n",
    "    lakefs.objects.upload_object(repository=repo_name, \n",
    "                                 branch=exp2_branch, \n",
    "                                 path=f\"{config_path}/config.json\", \n",
    "                                 content=f\n",
    "                                )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81022681",
   "metadata": {},
   "source": [
    "#### Load training data from lakeFS. \n",
    "#### Generate images and labels for training and Commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f3901",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_training_data(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764664de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Commit training data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "654d5b5d",
   "metadata": {},
   "source": [
    "#### Train the model. \n",
    "#### Upload model metrics to lakeFS and commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4e18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2, metrics2 = ml_pipeline(params, images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392eb6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_metrics(metrics2, repo_name, params['metrics_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a62439",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['loss'], params['accuracy'] = load_metrics(repo_name, params['metrics_path'])\n",
    "pprint.pprint(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c743e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefs.branches.diff_branch(repository=repo_name, \n",
    "                            branch=exp2_branch).results\n",
    "\n",
    "commit_meta_params = {}\n",
    "for k,v in params.items():\n",
    "    commit_meta_params[k]=str(v)\n",
    "pprint.pprint(commit_meta_params)\n",
    "\n",
    "lakefs.commits.commit(repository=repo_name,\n",
    "                      branch=exp2_branch,\n",
    "                      commit_creation=CommitCreation(\n",
    "                          message=f\"Saving model metrics to {exp2_branch}\",\n",
    "                          metadata=commit_meta_params)\n",
    "                     )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8e983b4",
   "metadata": {},
   "source": [
    "#### Upload model artifacts to lakeFS and commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c0477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save(model2, \n",
    "           params['model_name'], \n",
    "           params['repo_name'], \n",
    "           params['artifacts_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445f4d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefs.branches.diff_branch(repository=repo_name, \n",
    "                            branch=exp2_branch).results\n",
    "\n",
    "commit_meta_params = {}\n",
    "for k,v in params.items():\n",
    "    commit_meta_params[k]=str(v)\n",
    "pprint.pprint(commit_meta_params)\n",
    "\n",
    "lakefs.commits.commit(repository=repo_name,\n",
    "                      branch=exp2_branch,\n",
    "                      commit_creation=CommitCreation(\n",
    "                          message=f\"Saving model artifacts to {exp2_branch}\",\n",
    "                          metadata=commit_meta_params)\n",
    "                     )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e262d9e6",
   "metadata": {},
   "source": [
    "#### Load the pickle file from lakeFS, and run predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855af9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_reloaded = model_load(params['model_name'], \n",
    "           params['repo_name'], \n",
    "           params['artifacts_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db225f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = split_train_test(images, labels, params['train_test_split_ratio'])\n",
    "pred = model2_reloaded.predict(x_test)\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2146e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (19 , 10))\n",
    "n = 0 \n",
    "\n",
    "for i in range(9):\n",
    "    n += 1 \n",
    "    r = np.random.randint( 0, x_test.shape[0], 1)\n",
    "    \n",
    "    plt.subplot(3, 3, n)\n",
    "    plt.subplots_adjust(hspace = 0.3, wspace = 0.3)\n",
    "    \n",
    "    plt.imshow(x_test[r[0]])\n",
    "    plt.title('Actual = {}, Predicted = {}'.format(y_test[r[0]] , y_test[r[0]]*pred[r[0]][y_test[r[0]]]) )\n",
    "    plt.xticks([]) , plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47198a92",
   "metadata": {},
   "source": [
    "### Compare in both branches. Merge the winning model to Prod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e7fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_branch = exp2_branch\n",
    "if metrics1['accuracy']> metrics2['accuracy']:\n",
    "    win_branch = exp1_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c824b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b02637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefs.refs.merge_into_branch(repository=repo_name, \n",
    "                              source_ref=win_branch, \n",
    "                              destination_branch=prod_branch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c72748a4",
   "metadata": {},
   "source": [
    "## Reproducing ML experiments with lakeFS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabb2e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_branch = exp1_branch\n",
    "tag = f'{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}_{tag_branch}'\n",
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51bde85",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefs.tags.create_tag(\n",
    "    repository=repo_name,\n",
    "    tag_creation=TagCreation(\n",
    "        id=tag, \n",
    "        ref=tag_branch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78acf96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_tag ={\n",
    "    'repo_name': repo_name,\n",
    "    'branch': tag,\n",
    "    'image_path': f\"{tag}/{raw_path}/{images_path}\",\n",
    "    'artifacts_path': f\"{tag}/{artifact_path}\",\n",
    "    'metrics_path': f\"{tag}/{metrics_path}\",\n",
    "    'model_name': \"model.pkl\",\n",
    "    'delimiter': \"/\",\n",
    "    'n_cats': 3,\n",
    "    'n_images': 50,\n",
    "    'is_shuffle': True,\n",
    "    'is_normalize': True,\n",
    "    'epochs': 10,\n",
    "    'train_test_split_ratio': 0.15,\n",
    "    'optimizer': \"adagrad\",\n",
    "    'loss': \"sparse_categorical_crossentropy\",\n",
    "    'metrics': [\"accuracy\"]\n",
    "}\n",
    "pprint.pprint(params_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f06a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_training_data(params_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2212901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = preprocess(images, labels, params['is_shuffle'], params['is_normalize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a227a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_model_reloaded = model_load(params_tag['model_name'], \n",
    "           params_tag['repo_name'], \n",
    "           params_tag['artifacts_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba2df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tag_model_reloaded.predict(images)\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efbce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (19 , 10))\n",
    "n = 0 \n",
    "\n",
    "for i in range(9):\n",
    "    n += 1 \n",
    "    r = np.random.randint( 0, x_test.shape[0], 1)\n",
    "    \n",
    "    plt.subplot(3, 3, n)\n",
    "    plt.subplots_adjust(hspace = 0.3, wspace = 0.3)\n",
    "    \n",
    "    plt.imshow(x_test[r[0]])\n",
    "    plt.title('Actual = {}, Predicted = {}'.format(y_test[r[0]] , y_test[r[0]]*pred[r[0]][y_test[r[0]]]) )\n",
    "    plt.xticks([]) , plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb0a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d968288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77f5674e",
   "metadata": {},
   "source": [
    "## DONE!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
